{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/anatole/source/css/blog_basic.css","path":"css/blog_basic.css","modified":0,"renderable":1},{"_id":"themes/anatole/source/css/font-awesome.min.css","path":"css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/anatole/source/css/style.css","path":"css/style.css","modified":0,"renderable":1},{"_id":"themes/anatole/source/css/style.scss","path":"css/style.scss","modified":0,"renderable":1},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.eot","path":"fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.woff","path":"fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/anatole/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/anatole/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/anatole/source/images/logo@2x.png","path":"images/logo@2x.png","modified":0,"renderable":1},{"_id":"themes/anatole/source/js/jquery-migrate-1.2.1.min.js","path":"js/jquery-migrate-1.2.1.min.js","modified":0,"renderable":1},{"_id":"themes/anatole/source/js/jquery.appear.js","path":"js/jquery.appear.js","modified":0,"renderable":1},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.ttf","path":"fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/anatole/source/js/jquery.js","path":"js/jquery.js","modified":0,"renderable":1},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.svg","path":"fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/anatole/source/js/shader.js","path":"js/shader.js","modified":0,"renderable":1}],"Cache":[{"_id":"themes/anatole/.gitignore","hash":"9fcbb05b158b5d3ba1ee60d38889e05e8b0f3f2a","modified":1585704166575},{"_id":"themes/anatole/LICENSE","hash":"359cb81298c1fdbccf531548fc097466b0151be4","modified":1585704166575},{"_id":"themes/anatole/README.md","hash":"88d58e69eda2435ce074bc6cafb1817346089a88","modified":1585704166575},{"_id":"themes/anatole/_config.sample.yml","hash":"c6bce49d93a37e9ed5f0c4b104decc0bd7832e04","modified":1585704166575},{"_id":"themes/anatole/package.json","hash":"cf731a3ebf3913747fccb6c4c6615eba7d7e88a4","modified":1585704166575},{"_id":"source/_posts/hello-world.md","hash":"f2dfa89600b7a4b330492cea174511be89aa615d","modified":1585752124339},{"_id":"themes/anatole/languages/pt-BR.yml","hash":"ac5a0003d7793aade51f70643b2e2a631b6a65c3","modified":1585704166575},{"_id":"themes/anatole/languages/zh-cn.yml","hash":"dab1823e036f4adb3b7ae9efe95a37a15e47bec2","modified":1585704166575},{"_id":"themes/anatole/layout/archive.pug","hash":"1a161404966000b25a84762f08da3ca60af146bc","modified":1585704166575},{"_id":"themes/anatole/layout/category.pug","hash":"ae894ef4baee4a0c7c8e66641166061e789f1fa7","modified":1585704166575},{"_id":"themes/anatole/layout/index.pug","hash":"53da3cfd498951148acb33de1574df80ae282dc5","modified":1585704166575},{"_id":"themes/anatole/layout/mixins.pug","hash":"107bfd2fe10de0d8b110c13bb1168af79e62a37c","modified":1585704166575},{"_id":"themes/anatole/layout/page.pug","hash":"15142c94e5c2247aba8efb64cffd695b3f133670","modified":1585704166575},{"_id":"themes/anatole/layout/post.pug","hash":"012f8e272cf4713a0f08e3b702dab08ef6593b31","modified":1585704166575},{"_id":"themes/anatole/layout/tag.pug","hash":"7017a8bae4f4a412dafb556772bdcf2cfddeb79f","modified":1585704166575},{"_id":"themes/anatole/layout/partial/comments.pug","hash":"fad5bbe7c2a134c892fcb1c731d979463145a49b","modified":1585704166575},{"_id":"themes/anatole/layout/partial/footer.pug","hash":"772f03a2dd9fa4fed422c4eb4d5d1ace84c65580","modified":1585704166575},{"_id":"themes/anatole/layout/partial/head.pug","hash":"3d6126d0b53a889a46ad61eb8cc0ab1b35446c49","modified":1585704166575},{"_id":"themes/anatole/layout/partial/layout.pug","hash":"d06c97b36de345908a9b0c081d84aed9f655fd9d","modified":1585771846647},{"_id":"themes/anatole/layout/partial/nav.pug","hash":"03199bf8d18b419acb9779931007d182e782f18f","modified":1585867611168},{"_id":"themes/anatole/layout/partial/sidebar.pug","hash":"6721b1e0b1e59e54d031a4978e7f82aece336efc","modified":1585704166575},{"_id":"themes/anatole/source/css/blog_basic.css","hash":"7db88f0873e858b21a4c981c8481708ad9117478","modified":1585704166575},{"_id":"themes/anatole/source/css/font-awesome.min.css","hash":"95d0b0c4d11105c81be1857b744076a1d2bed918","modified":1585704166575},{"_id":"themes/anatole/source/css/style.css","hash":"5d49b84a107e508b5e63b4ac0b1c9f1900be93dc","modified":1585867877225},{"_id":"themes/anatole/source/css/style.scss","hash":"175c9237798207f705c62777b5f51509adec6d0a","modified":1585704166575},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.eot","hash":"0183979056f0b87616cd99d5c54a48f3b771eee6","modified":1585704166575},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.woff","hash":"7d65e0227d0d7cdc1718119cd2a7dce0638f151c","modified":1585704166579},{"_id":"themes/anatole/source/images/favicon.png","hash":"790e72ae1fc16ce1c33af13d19935ab6267eba38","modified":1586038703595},{"_id":"themes/anatole/source/images/logo.png","hash":"41bdf2ebd8cc193ad82a211758af827d891b1a90","modified":1585704166579},{"_id":"themes/anatole/source/images/logo@2x.png","hash":"49c3fa97724abf53861bf11356ea9ba5bcb11576","modified":1585704166579},{"_id":"themes/anatole/source/js/jquery-migrate-1.2.1.min.js","hash":"743052320809514fb788fe1d3df37fc87ce90452","modified":1585704166579},{"_id":"themes/anatole/source/js/jquery.appear.js","hash":"1f8067d7bd4c0bde30785e8016100f239e14394f","modified":1585704166579},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.ttf","hash":"6225ccc4ec94d060f19efab97ca42d842845b949","modified":1585704166579},{"_id":"themes/anatole/source/js/jquery.js","hash":"bfc05b695dfa4f23e11d04b84993585da7a764bf","modified":1585704166579},{"_id":"themes/anatole/source/fonts/fontawesome-webfont.svg","hash":"cd980eab6db5fa57db670cb2e4278e67e1a4d6c9","modified":1585704166575},{"_id":"source/about/index.md","hash":"2e3c4f38d236526d89e933a0692c65ff582b2a3c","modified":1586038317050},{"_id":"source/archives/index.md","hash":"c9ebba08584753c0b8c9d4a594accb436ae424b3","modified":1585751573156},{"_id":"source/links/index.md","hash":"afe5d6185c45ff602f25e8f1060571f9e790ac21","modified":1585756967952},{"_id":"themes/anatole/languages/en.yml","hash":"4d442c64e1c3f29771afc660dde378462a1c4606","modified":1586039085761},{"_id":"public/about/index.html","hash":"95ca69e63bb011eb0db2256714cc77873cf674a6","modified":1586038338230},{"_id":"public/archives/index.html","hash":"7e8e02725ea2c23d0dd93b6ef6f4ff525c6cd67b","modified":1586038338230},{"_id":"public/links/index.html","hash":"c68c34e95502844a1c8c0cf9dad0fedc1f8528be","modified":1586038338230},{"_id":"public/2020/04/01/hello-world/index.html","hash":"7aee267649879db2dfe5112e10efd9a1c54f8edc","modified":1585755865623},{"_id":"public/index.html","hash":"92f07b3fb8eee36955334871a2b044b82c5b8eac","modified":1586038338230},{"_id":"public/css/style.scss","hash":"175c9237798207f705c62777b5f51509adec6d0a","modified":1585771347540},{"_id":"public/fonts/fontawesome-webfont.eot","hash":"0183979056f0b87616cd99d5c54a48f3b771eee6","modified":1585771347540},{"_id":"public/images/favicon.png","hash":"790e72ae1fc16ce1c33af13d19935ab6267eba38","modified":1586038732191},{"_id":"public/images/logo.png","hash":"41bdf2ebd8cc193ad82a211758af827d891b1a90","modified":1585771347540},{"_id":"public/images/logo@2x.png","hash":"49c3fa97724abf53861bf11356ea9ba5bcb11576","modified":1585771347540},{"_id":"public/fonts/fontawesome-webfont.woff","hash":"7d65e0227d0d7cdc1718119cd2a7dce0638f151c","modified":1585771347540},{"_id":"public/fonts/fontawesome-webfont.ttf","hash":"6225ccc4ec94d060f19efab97ca42d842845b949","modified":1585771347540},{"_id":"public/css/blog_basic.css","hash":"7db88f0873e858b21a4c981c8481708ad9117478","modified":1585771347540},{"_id":"public/css/style.css","hash":"5d49b84a107e508b5e63b4ac0b1c9f1900be93dc","modified":1585867921346},{"_id":"public/js/jquery-migrate-1.2.1.min.js","hash":"743052320809514fb788fe1d3df37fc87ce90452","modified":1585771347540},{"_id":"public/js/jquery.appear.js","hash":"1f8067d7bd4c0bde30785e8016100f239e14394f","modified":1585771347540},{"_id":"public/css/font-awesome.min.css","hash":"95d0b0c4d11105c81be1857b744076a1d2bed918","modified":1585771347540},{"_id":"public/js/jquery.js","hash":"bfc05b695dfa4f23e11d04b84993585da7a764bf","modified":1585771347540},{"_id":"public/fonts/fontawesome-webfont.svg","hash":"cd980eab6db5fa57db670cb2e4278e67e1a4d6c9","modified":1585771347540},{"_id":"source/_posts/number2image.md","hash":"a20641fa59eed37500ab7f540bab9f4496a7ec06","modified":1586035601532},{"_id":"source/_posts/number2image/pattern.png","hash":"bfcf276e31b2d52e5294a0d0bb886da758b562f2","modified":1585756051396},{"_id":"public/2020/04/01/number2image/index.html","hash":"77fc45173d850adfba9ed0e0d1e107806ff28e8e","modified":1585867631383},{"_id":"public/2020/04/01/number2image/pattern.png","hash":"bfcf276e31b2d52e5294a0d0bb886da758b562f2","modified":1585771347540},{"_id":"source/jardin-sonoro/index.md","hash":"c339c01abae539bec2d912bdbc733311e12578ae","modified":1585757418186},{"_id":"source/_posts/Singing-Synthesis.md","hash":"1d16ea0827f54b48e37e91cb990cd90c4ab26a78","modified":1586033281403},{"_id":"source/_posts/jardin-sonoro.md","hash":"a461256472284bd04657a609c38f546c8c23ef95","modified":1585758297890},{"_id":"themes/anatole/source/js/shader.js","hash":"7d1f7b15448bf9ab3b37c4c61e8e70d2472afef7","modified":1585771193020},{"_id":"source/_posts/jardin-sonoro/player.png","hash":"607e15d256b4e50cd3d1d9f6ceab76804615c3ea","modified":1585758372468},{"_id":"source/_posts/jardin-sonoro/inicio.png","hash":"1e1f7d5c7cae7e42a21b0e3e20ceb2bb82e1fadc","modified":1585758451406},{"_id":"source/_posts/jardin-sonoro/load.png","hash":"8ca07eaef8f075a910670654055475ec8d5e15c6","modified":1585757745593},{"_id":"public/2020/04/01/Singing-Synthesis/index.html","hash":"3db173a7dc6194179cf09aa7e976ec4e4a866b55","modified":1585867631383},{"_id":"public/2019/01/03/jardin-sonoro/index.html","hash":"4beb8c5fad89c21431986c79a32d8665cd4f570e","modified":1586038338230},{"_id":"public/2019/01/03/jardin-sonoro/player.png","hash":"607e15d256b4e50cd3d1d9f6ceab76804615c3ea","modified":1585771347540},{"_id":"public/js/shader.js","hash":"7d1f7b15448bf9ab3b37c4c61e8e70d2472afef7","modified":1585771347540},{"_id":"public/2019/01/03/jardin-sonoro/inicio.png","hash":"1e1f7d5c7cae7e42a21b0e3e20ceb2bb82e1fadc","modified":1585771347540},{"_id":"public/2019/01/03/jardin-sonoro/load.png","hash":"8ca07eaef8f075a910670654055475ec8d5e15c6","modified":1585771347540},{"_id":"source/_posts/AI-Poem-Writer.md","hash":"4307770130e900a2822398693174d421c2884d0a","modified":1586034899306},{"_id":"source/_posts/Audio-Reactive-Slime.md","hash":"3e0397421963d37cd18229520958f21f87afe2b4","modified":1586033675912},{"_id":"source/_posts/Midi-to-Voice.md","hash":"96ab7c6488cdee9742413f5b2252644a5f828993","modified":1586033150152},{"_id":"source/_posts/Normalized-Google-Distance.md","hash":"7dca6aea9efc854374ca9b1cbf0d7083fdbc33da","modified":1586035783196},{"_id":"source/_posts/Regenerative-cellular-automata.md","hash":"bbb43408d204158108a87297d5c364c440f28e1e","modified":1586035554888},{"_id":"source/_posts/Scraping-formatted-text-from-images.md","hash":"49e04bca0790dbdaf42d5b5bc5bda0eba992ca2d","modified":1586035482881},{"_id":"source/_posts/bu3nAmigue-Experimental-Indie-Band.md","hash":"df3fcdb9fcadcd2ca2e86faef9b1ecd545af6f53","modified":1586034273796},{"_id":"source/_posts/Style-Transfer-Experiments.md","hash":"157023093bc93dbafc2c990fabd6c71a35dad793","modified":1586036249799},{"_id":"source/reviews/index.md","hash":"560910bc59a6aab542bcaaceca0c81eaf3b89ea2","modified":1585874782813},{"_id":"public/reviews/index.html","hash":"63d2f60c3e7845727d19ea7a89b4cb1ff0a76f8c","modified":1586038338230},{"_id":"public/2020/04/02/Regenerative-cellular-automata/index.html","hash":"46bb91e665d2401c88b769351dd1460739a66d17","modified":1585867631383},{"_id":"public/2020/04/02/Scraping-formatted-text-from-images/index.html","hash":"039eda95a814123e87778f16a028365e2558ef87","modified":1585867631383},{"_id":"public/2020/04/02/Normalized-Google-Distance/index.html","hash":"e829f8284ce5149fdb63d076ccb59561fe1b2947","modified":1585867631383},{"_id":"public/2020/04/02/AI-Poem-Writer/index.html","hash":"5295efd48b3b3dd155d48526751e9b92687c671d","modified":1585867631383},{"_id":"public/2020/04/02/bu3nAmigue-Experimental-Indie-Band/index.html","hash":"265a6f901f0701eaff7574407232affdf863d0e7","modified":1585867631383},{"_id":"public/2020/04/02/Midi-to-Voice/index.html","hash":"3a460554a31795be47719b720eb61c7a01932bf4","modified":1585867631383},{"_id":"public/2020/04/02/Style-Transfer-Experiments/index.html","hash":"28de2a86c4858f6d1194cf041879c0eda6c2b59b","modified":1585867631383},{"_id":"public/2020/04/02/Audio-Reactive-Slime/index.html","hash":"a34cba49dbf9c523cd1d66b458f2f6ad5f2ec6a7","modified":1585867631383},{"_id":"public/page/2/index.html","hash":"111c7a461f0017717958c9b9894f270ad28ca3d5","modified":1586038338230},{"_id":"source/reviews/index/iframe_height.js","hash":"4245eaba9341942b656b28138230cb810a9ce5ef","modified":1585870633225},{"_id":"public/reviews/index/iframe_height.js","hash":"4245eaba9341942b656b28138230cb810a9ce5ef","modified":1585871543201},{"_id":"source/_posts/Coding-Psychological-Experiments.md","hash":"950ccc2683bb721273706d1ab4635b359ea6515d","modified":1586050185635},{"_id":"source/_posts/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis.md","hash":"12f28bd9bfd879fdf4e43b72ae214a55e63738bf","modified":1586050370974},{"_id":"source/_posts/Looking-for-the-beauty-formula.md","hash":"85d44f38dacaae8021e7591bcbe6f50a5bac22aa","modified":1586049903310},{"_id":"source/_posts/Scraping-drum-patterns-from-PDF.md","hash":"f08537d6c8998b747cbc2ac4e7703c4970aebce2","modified":1586050243296},{"_id":"source/_posts/AI-Poem-Writer/english3.jpg","hash":"a16013173af15c4f4260a8764c97b26934792eca","modified":1586034741748},{"_id":"source/_posts/AI-Poem-Writer/spanish1.jpg","hash":"38a6c8212230b45da28041539c701de2a8fe4d7c","modified":1586034780239},{"_id":"source/_posts/AI-Poem-Writer/spanish2.jpg","hash":"66663035a179be5012e4a1c79feadda0eb92b05f","modified":1586034777463},{"_id":"source/_posts/AI-Poem-Writer/spanish3.jpg","hash":"6df3668cb8bab723a2563455b555dc4cc9ec505c","modified":1586034774083},{"_id":"source/_posts/Audio-Reactive-Slime/physarum.jpg","hash":"a0d9991e8334d1f4e22dff04d25decdd48ce696f","modified":1586033422066},{"_id":"source/_posts/Midi-to-Voice/shallow.jpg","hash":"d2d8b69ab4c90ea7ba9d30163ecbdc4dcb732b77","modified":1586032851395},{"_id":"source/_posts/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis/emotiv.jpg","hash":"43cc364622a072c5e6946d1cd0c62b81ad663a1a","modified":1586037559171},{"_id":"source/_posts/AI-Poem-Writer/english1.jpg","hash":"9562bb8d18e1f8edfdbc08d8e3c95aa7e3104cbb","modified":1586034770523},{"_id":"source/_posts/AI-Poem-Writer/english2.jpg","hash":"0bc0de5bffbf90d9257518770e9b14c271ebc269","modified":1586034765659},{"_id":"source/_posts/Audio-Reactive-Slime/physarum2.jpg","hash":"724cb8e960f7ea48cd14da16d17421aaf64c9d88","modified":1586033434118},{"_id":"source/_posts/Style-Transfer-Experiments/s1_1.jpg","hash":"38048f18357f46490e0dd0a4415b5bf3e7e14742","modified":1586036046341},{"_id":"source/_posts/Style-Transfer-Experiments/s1_2.jpg","hash":"1a92cead449c6b8536575a67a8089ddc6bcc20fd","modified":1586036063718},{"_id":"source/_posts/Style-Transfer-Experiments/s1_3.jpg","hash":"8432759683d38472b0164195933a72f2e398a8bc","modified":1586036066786},{"_id":"source/_posts/Style-Transfer-Experiments/s1_4.jpg","hash":"c4a164db4f1425be609bdbf77eac3aaf23304541","modified":1586036073899},{"_id":"source/_posts/Style-Transfer-Experiments/s2_4.jpg","hash":"2e17137eaba439cf2aa13ecd9c7bcf8cefdb913e","modified":1586036093948},{"_id":"source/_posts/Style-Transfer-Experiments/s2_1.jpg","hash":"8528221de84b2dd33c5d0a1897da8677c81d60f1","modified":1586036082635},{"_id":"source/_posts/Style-Transfer-Experiments/s3_2.jpg","hash":"f2c6897389095feedddac8eb56a19079b35960f4","modified":1586036102472},{"_id":"source/_posts/Style-Transfer-Experiments/s2_3.jpg","hash":"9f80e97f85c3bdf424ecc3f0403599a0984c86e2","modified":1586036090643},{"_id":"source/_posts/Style-Transfer-Experiments/s2_2.jpg","hash":"c213f306f70f2686f22fa9ce66f315826e08f5df","modified":1586036086307},{"_id":"source/_posts/Style-Transfer-Experiments/s4_2.jpg","hash":"3b38cb19c1adcad4e36ecfbe3a40d96175034d74","modified":1586036109361},{"_id":"source/_posts/Style-Transfer-Experiments/s3_1.jpg","hash":"45617811d73e223e806f8ae918c003f176b96391","modified":1586036099536},{"_id":"source/_posts/Style-Transfer-Experiments/s4_1.jpg","hash":"cb7d6f1f0bdba452dfa7db1694c7f8620199b829","modified":1586036105460},{"_id":"source/_posts/Regenerative-cellular-automata/automata2.gif","hash":"83a4cc0247ae57fd14548057f978b608412a879b","modified":1586035296618},{"_id":"source/_posts/Regenerative-cellular-automata/automata1.gif","hash":"e7dec8f3ad402fe695aecf06b74ed4862862a0ce","modified":1586035178812},{"_id":"public/2020/03/24/Regenerative-cellular-automata/index.html","hash":"5aaf1867f089a04bc8f9dc422ed89f23c0df068b","modified":1586038338230},{"_id":"public/2020/02/20/Scraping-drum-patterns-from-PDF/index.html","hash":"e6f0f797982b59be1773788f2d6032f3f20ec50f","modified":1586050397682},{"_id":"public/2020/01/21/AI-Poem-Writer/index.html","hash":"2e44e1fa7a6b1cf7c6e4210414b46cf83f33da68","modified":1586038338230},{"_id":"public/2019/12/31/Audio-Reactive-Slime/index.html","hash":"7dbb3406ae893cb2a46a47dfb761346c05a58588","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/index.html","hash":"2b5b8cd279386fd4c25076cafe7fcf01259da34f","modified":1586038338230},{"_id":"public/2019/12/15/bu3nAmigue-Experimental-Indie-Band/index.html","hash":"0e226e4a355926b42d184806ec12215337c5dc0e","modified":1586038338230},{"_id":"public/2019/10/03/Scraping-formatted-text-from-images/index.html","hash":"b49c3912612d08ee7d5dd61ae72114642f9bcf72","modified":1586038338230},{"_id":"public/2019/08/05/Looking-for-the-beauty-formula/index.html","hash":"107143ac103ddc5ac174900eb875e5c3eb07e53b","modified":1586050397682},{"_id":"public/2019/06/09/Coding-Psychological-Experiments/index.html","hash":"e5f1a9b3b34c24e6bd38b32d1539634b329a8b8e","modified":1586050397682},{"_id":"public/2019/06/09/Normalized-Google-Distance/index.html","hash":"66cb041bc6263345e3f85113001655a1f8a94acb","modified":1586038338230},{"_id":"public/2019/06/07/number2image/index.html","hash":"d422dad10c46b6ee63fcda62c2d76b7d6a410150","modified":1586038338230},{"_id":"public/2019/05/05/Singing-Synthesis/index.html","hash":"4c68c34243e5e9914800b9023e42113825c0123a","modified":1586038338230},{"_id":"public/2018/06/10/Midi-to-Voice/index.html","hash":"9c1391ebe6837df2b70911d4a2c60542082b6c77","modified":1586038338230},{"_id":"public/2018/04/04/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis/index.html","hash":"9d63cfbc36f7e98345b5b1b5610cb7785e9f45c5","modified":1586050397682},{"_id":"public/tags/poetry-gpt-2/index.html","hash":"ab9dd23dd88ec215470acd4449f2c005c6606c68","modified":1586038338230},{"_id":"public/tags/ocr/index.html","hash":"efdd24c75d4ac60c4544b7a1c4d01f7945585db7","modified":1586038338230},{"_id":"public/tags/style-transfer/index.html","hash":"5bf0c141ec47796e61f22a4341fad37b8e09d4af","modified":1586038338230},{"_id":"public/tags/cellular-automata/index.html","hash":"691fba45d8d673ad2915eed06f98f9192b5150a7","modified":1586038338230},{"_id":"public/tags/neuroscience/index.html","hash":"d926035ce5bf3509db08a7f731e08d4fecd4d1bd","modified":1586038338230},{"_id":"public/2019/12/31/Audio-Reactive-Slime/physarum.jpg","hash":"a0d9991e8334d1f4e22dff04d25decdd48ce696f","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/english3.jpg","hash":"a16013173af15c4f4260a8764c97b26934792eca","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/spanish3.jpg","hash":"6df3668cb8bab723a2563455b555dc4cc9ec505c","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/spanish2.jpg","hash":"66663035a179be5012e4a1c79feadda0eb92b05f","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/spanish1.jpg","hash":"38a6c8212230b45da28041539c701de2a8fe4d7c","modified":1586038338230},{"_id":"public/2018/04/04/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis/emotiv.jpg","hash":"43cc364622a072c5e6946d1cd0c62b81ad663a1a","modified":1586038338230},{"_id":"public/2018/06/10/Midi-to-Voice/shallow.jpg","hash":"d2d8b69ab4c90ea7ba9d30163ecbdc4dcb732b77","modified":1586038338230},{"_id":"public/2019/12/31/Audio-Reactive-Slime/physarum2.jpg","hash":"724cb8e960f7ea48cd14da16d17421aaf64c9d88","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/english2.jpg","hash":"0bc0de5bffbf90d9257518770e9b14c271ebc269","modified":1586038338230},{"_id":"public/2020/01/21/AI-Poem-Writer/english1.jpg","hash":"9562bb8d18e1f8edfdbc08d8e3c95aa7e3104cbb","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s1_2.jpg","hash":"1a92cead449c6b8536575a67a8089ddc6bcc20fd","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s1_3.jpg","hash":"8432759683d38472b0164195933a72f2e398a8bc","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s1_4.jpg","hash":"c4a164db4f1425be609bdbf77eac3aaf23304541","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s2_2.jpg","hash":"c213f306f70f2686f22fa9ce66f315826e08f5df","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s2_3.jpg","hash":"9f80e97f85c3bdf424ecc3f0403599a0984c86e2","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s2_4.jpg","hash":"2e17137eaba439cf2aa13ecd9c7bcf8cefdb913e","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s1_1.jpg","hash":"38048f18357f46490e0dd0a4415b5bf3e7e14742","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s2_1.jpg","hash":"8528221de84b2dd33c5d0a1897da8677c81d60f1","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s3_2.jpg","hash":"f2c6897389095feedddac8eb56a19079b35960f4","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s4_2.jpg","hash":"3b38cb19c1adcad4e36ecfbe3a40d96175034d74","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s3_1.jpg","hash":"45617811d73e223e806f8ae918c003f176b96391","modified":1586038338230},{"_id":"public/2019/12/29/Style-Transfer-Experiments/s4_1.jpg","hash":"cb7d6f1f0bdba452dfa7db1694c7f8620199b829","modified":1586038338230},{"_id":"public/2020/03/24/Regenerative-cellular-automata/automata2.gif","hash":"83a4cc0247ae57fd14548057f978b608412a879b","modified":1586038338230},{"_id":"public/2019/06/07/number2image/pattern.png","hash":"bfcf276e31b2d52e5294a0d0bb886da758b562f2","modified":1586038338230},{"_id":"public/2020/03/24/Regenerative-cellular-automata/automata1.gif","hash":"e7dec8f3ad402fe695aecf06b74ed4862862a0ce","modified":1586038338230}],"Category":[],"Data":[],"Page":[{"title":"about","_content":"\nI am a freelance software developer from Argentina specialized in Data Science and Creative Coding. I have a computer science degree and several years of experience as a programmer and math teacher.\n\nYou can check my resume and reviews at [codementor](https://www.codementor.io/mathiasgatti). Several tools and projects I develope are open source and available [here](https://github.com/mathigatti). In my spare time I contribute on open source projects like [FoxDot](https://github.com/Qirky/FoxDot/graphs/contributors), a real time music composition framework.\n\nI take part into [bu3nAmigue](https://www.instagram.com/bu3namigue/), an artistic collective that applies technology to the arts.\n\nYou can contact me through [Instagram](https://instagram.com/mathigatti), [facebook](https://facebook.com/mathi.gatti) or e-mail (mathigatti(a)gmail.com). You can hire me through [codementor](https://www.codementor.io/mathiasgatti).","source":"about/index.md","raw":"---\ntitle: about\n---\n\nI am a freelance software developer from Argentina specialized in Data Science and Creative Coding. I have a computer science degree and several years of experience as a programmer and math teacher.\n\nYou can check my resume and reviews at [codementor](https://www.codementor.io/mathiasgatti). Several tools and projects I develope are open source and available [here](https://github.com/mathigatti). In my spare time I contribute on open source projects like [FoxDot](https://github.com/Qirky/FoxDot/graphs/contributors), a real time music composition framework.\n\nI take part into [bu3nAmigue](https://www.instagram.com/bu3namigue/), an artistic collective that applies technology to the arts.\n\nYou can contact me through [Instagram](https://instagram.com/mathigatti), [facebook](https://facebook.com/mathi.gatti) or e-mail (mathigatti(a)gmail.com). You can hire me through [codementor](https://www.codementor.io/mathiasgatti).","date":"2020-04-04T22:11:57.050Z","updated":"2020-04-04T22:11:57.050Z","path":"about/index.html","_id":"ck8hfk5ch0000vrry173h9qte","comments":1,"layout":"page","content":"<p>I am a freelance software developer from Argentina specialized in Data Science and Creative Coding. I have a computer science degree and several years of experience as a programmer and math teacher.</p>\n<p>You can check my resume and reviews at <a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>. Several tools and projects I develope are open source and available <a href=\"https://github.com/mathigatti\" target=\"_blank\" rel=\"noopener\">here</a>. In my spare time I contribute on open source projects like <a href=\"https://github.com/Qirky/FoxDot/graphs/contributors\" target=\"_blank\" rel=\"noopener\">FoxDot</a>, a real time music composition framework.</p>\n<p>I take part into <a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">bu3nAmigue</a>, an artistic collective that applies technology to the arts.</p>\n<p>You can contact me through <a href=\"https://instagram.com/mathigatti\" target=\"_blank\" rel=\"noopener\">Instagram</a>, <a href=\"https://facebook.com/mathi.gatti\" target=\"_blank\" rel=\"noopener\">facebook</a> or e-mail (mathigatti(a)gmail.com). You can hire me through <a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>I am a freelance software developer from Argentina specialized in Data Science and Creative Coding. I have a computer science degree and several years of experience as a programmer and math teacher.</p>\n<p>You can check my resume and reviews at <a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>. Several tools and projects I develope are open source and available <a href=\"https://github.com/mathigatti\" target=\"_blank\" rel=\"noopener\">here</a>. In my spare time I contribute on open source projects like <a href=\"https://github.com/Qirky/FoxDot/graphs/contributors\" target=\"_blank\" rel=\"noopener\">FoxDot</a>, a real time music composition framework.</p>\n<p>I take part into <a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">bu3nAmigue</a>, an artistic collective that applies technology to the arts.</p>\n<p>You can contact me through <a href=\"https://instagram.com/mathigatti\" target=\"_blank\" rel=\"noopener\">Instagram</a>, <a href=\"https://facebook.com/mathi.gatti\" target=\"_blank\" rel=\"noopener\">facebook</a> or e-mail (mathigatti(a)gmail.com). You can hire me through <a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>.</p>\n"},{"title":"archives","date":"2020-04-01T14:32:53.000Z","_content":"","source":"archives/index.md","raw":"---\ntitle: archives\ndate: 2020-04-01 11:32:53\n---\n","updated":"2020-04-01T14:32:53.156Z","path":"archives/index.html","comments":1,"layout":"page","_id":"ck8hfk5ck0001vrry5wkoh5ez","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"links","_content":"\n[linkedIn](https://www.linkedin.com/in/mathias-gatti-a607945b/)\n[codementor](https://www.codementor.io/mathiasgatti)\n[GitHub](https://github.com/mathigatti)\n[bu3nAmigue](https://www.instagram.com/bu3namigue/)\n[Instagram](https://instagram.com/mathigatti)","source":"links/index.md","raw":"---\ntitle: links\n---\n\n[linkedIn](https://www.linkedin.com/in/mathias-gatti-a607945b/)\n[codementor](https://www.codementor.io/mathiasgatti)\n[GitHub](https://github.com/mathigatti)\n[bu3nAmigue](https://www.instagram.com/bu3namigue/)\n[Instagram](https://instagram.com/mathigatti)","date":"2020-04-01T16:02:47.952Z","updated":"2020-04-01T16:02:47.952Z","path":"links/index.html","_id":"ck8hfk5co0002vrryexsj7uuo","comments":1,"layout":"page","content":"<p><a href=\"https://www.linkedin.com/in/mathias-gatti-a607945b/\" target=\"_blank\" rel=\"noopener\">linkedIn</a><br><a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a><br><a href=\"https://github.com/mathigatti\" target=\"_blank\" rel=\"noopener\">GitHub</a><br><a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">bu3nAmigue</a><br><a href=\"https://instagram.com/mathigatti\" target=\"_blank\" rel=\"noopener\">Instagram</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://www.linkedin.com/in/mathias-gatti-a607945b/\" target=\"_blank\" rel=\"noopener\">linkedIn</a><br><a href=\"https://www.codementor.io/mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a><br><a href=\"https://github.com/mathigatti\" target=\"_blank\" rel=\"noopener\">GitHub</a><br><a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">bu3nAmigue</a><br><a href=\"https://instagram.com/mathigatti\" target=\"_blank\" rel=\"noopener\">Instagram</a></p>\n"},{"title":"reviews","_content":"This is a sample of 15 reviews I received working as a freelance developer and teacher at [codementor](https://www.codementor.io/@mathiasgatti). You can recharge the website to see a new set of reviews.\n\n<iframe src=\"https://reviews.mathigatti.com/\" frameBorder=\"0\" scrolling=\"no\" width=\"100%\" height=\"1600px\"></iframe>\n","source":"reviews/index.md","raw":"---\ntitle: reviews\n---\nThis is a sample of 15 reviews I received working as a freelance developer and teacher at [codementor](https://www.codementor.io/@mathiasgatti). You can recharge the website to see a new set of reviews.\n\n<iframe src=\"https://reviews.mathigatti.com/\" frameBorder=\"0\" scrolling=\"no\" width=\"100%\" height=\"1600px\"></iframe>\n","date":"2020-04-03T00:46:22.813Z","updated":"2020-04-03T00:46:22.813Z","path":"reviews/index.html","_id":"ck8jb761g0001nlrybnj1cuyu","comments":1,"layout":"page","content":"<p>This is a sample of 15 reviews I received working as a freelance developer and teacher at <a href=\"https://www.codementor.io/@mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>. You can recharge the website to see a new set of reviews.</p>\n<iframe src=\"https://reviews.mathigatti.com/\" frameBorder=\"0\" scrolling=\"no\" width=\"100%\" height=\"1600px\"></iframe>\n","site":{"data":{}},"excerpt":"","more":"<p>This is a sample of 15 reviews I received working as a freelance developer and teacher at <a href=\"https://www.codementor.io/@mathiasgatti\" target=\"_blank\" rel=\"noopener\">codementor</a>. You can recharge the website to see a new set of reviews.</p>\n<iframe src=\"https://reviews.mathigatti.com/\" frameBorder=\"0\" scrolling=\"no\" width=\"100%\" height=\"1600px\"></iframe>\n"},{"_content":"function resizeIframe(obj) {\nobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n}\n","source":"reviews/index/iframe_height.js","raw":"function resizeIframe(obj) {\nobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n}\n","date":"2020-04-02T23:37:13.225Z","updated":"2020-04-02T23:37:13.225Z","path":"reviews/index/iframe_height.js","layout":"false","_id":"ck8jefa9l0000cgryenxehwwc","title":"","comments":1,"content":"function resizeIframe(obj) {\nobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n}\n","site":{"data":{}},"excerpt":"","more":"function resizeIframe(obj) {\nobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n}\n"}],"Post":[{"title":"Number to Image","date":"2019-06-07T16:55:34.000Z","_content":"\nIn the following [link](https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f) you can find a script to convert any number into a 2 dimensional pattern. The details about the implementation are [here](https://www.codementor.io/@mathiasgatti/the-beauty-formula-identifying-interesting-patterns-automatically-based-on-aesthetic-metrics-basic-clustering-example-with-scikit-learn-xka5d6do8).\n\n<img src=\"pattern.png\" width=\"100%\" height=\"10\" border=\"5\" />","source":"_posts/number2image.md","raw":"---\ntitle: Number to Image\ndate: 2019-06-07 13:55:34\n---\n\nIn the following [link](https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f) you can find a script to convert any number into a 2 dimensional pattern. The details about the implementation are [here](https://www.codementor.io/@mathiasgatti/the-beauty-formula-identifying-interesting-patterns-automatically-based-on-aesthetic-metrics-basic-clustering-example-with-scikit-learn-xka5d6do8).\n\n<img src=\"pattern.png\" width=\"100%\" height=\"10\" border=\"5\" />","slug":"number2image","published":1,"updated":"2020-04-04T21:26:41.532Z","_id":"ck8hi88zu000085ry9tkg2461","comments":1,"layout":"post","photos":[],"link":"","content":"<p>In the following <a href=\"https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f\" target=\"_blank\" rel=\"noopener\">link</a> you can find a script to convert any number into a 2 dimensional pattern. The details about the implementation are <a href=\"https://www.codementor.io/@mathiasgatti/the-beauty-formula-identifying-interesting-patterns-automatically-based-on-aesthetic-metrics-basic-clustering-example-with-scikit-learn-xka5d6do8\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<img src=\"pattern.png\" width=\"100%\" height=\"10\" border=\"5\" />","site":{"data":{}},"excerpt":"","more":"<p>In the following <a href=\"https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f\" target=\"_blank\" rel=\"noopener\">link</a> you can find a script to convert any number into a 2 dimensional pattern. The details about the implementation are <a href=\"https://www.codementor.io/@mathiasgatti/the-beauty-formula-identifying-interesting-patterns-automatically-based-on-aesthetic-metrics-basic-clustering-example-with-scikit-learn-xka5d6do8\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<img src=\"pattern.png\" width=\"100%\" height=\"10\" border=\"5\" />"},{"title":"Jardin Sonoro","date":"2019-01-03T03:00:00.000Z","_content":"On March 2019 I worked on an interactive app which allows people to walk through museums and public parks and receive notifications with descriptions, audios and images when they are close to some relevant place. The first version was exhibited at the Jardin Botanico of Buenos Aires under the title of \"Jardin Sonoro\". You can check and donwload the app [here](https://play.google.com/store/apps/details?id=com.jardinsonoro.buenosaires).\n\n<img src=\"inicio.png\" width=\"30%\" height=\"50%\"/>\n<img src=\"load.png\" width=\"30%\" height=\"50%\" />\n<img src=\"player.png\" width=\"30%\" height=\"50%\" />","source":"_posts/jardin-sonoro.md","raw":"---\ntitle: Jardin Sonoro\ndate: 01/03/2019\n---\nOn March 2019 I worked on an interactive app which allows people to walk through museums and public parks and receive notifications with descriptions, audios and images when they are close to some relevant place. The first version was exhibited at the Jardin Botanico of Buenos Aires under the title of \"Jardin Sonoro\". You can check and donwload the app [here](https://play.google.com/store/apps/details?id=com.jardinsonoro.buenosaires).\n\n<img src=\"inicio.png\" width=\"30%\" height=\"50%\"/>\n<img src=\"load.png\" width=\"30%\" height=\"50%\" />\n<img src=\"player.png\" width=\"30%\" height=\"50%\" />","slug":"jardin-sonoro","published":1,"updated":"2020-04-01T16:24:57.890Z","_id":"ck8hj4b4c00024zry4lyp1yut","comments":1,"layout":"post","photos":[],"link":"","content":"<p>On March 2019 I worked on an interactive app which allows people to walk through museums and public parks and receive notifications with descriptions, audios and images when they are close to some relevant place. The first version was exhibited at the Jardin Botanico of Buenos Aires under the title of “Jardin Sonoro”. You can check and donwload the app <a href=\"https://play.google.com/store/apps/details?id=com.jardinsonoro.buenosaires\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<img src=\"inicio.png\" width=\"30%\" height=\"50%\"/>\n<img src=\"load.png\" width=\"30%\" height=\"50%\" />\n<img src=\"player.png\" width=\"30%\" height=\"50%\" />","site":{"data":{}},"excerpt":"","more":"<p>On March 2019 I worked on an interactive app which allows people to walk through museums and public parks and receive notifications with descriptions, audios and images when they are close to some relevant place. The first version was exhibited at the Jardin Botanico of Buenos Aires under the title of “Jardin Sonoro”. You can check and donwload the app <a href=\"https://play.google.com/store/apps/details?id=com.jardinsonoro.buenosaires\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<img src=\"inicio.png\" width=\"30%\" height=\"50%\"/>\n<img src=\"load.png\" width=\"30%\" height=\"50%\" />\n<img src=\"player.png\" width=\"30%\" height=\"50%\" />"},{"title":"Singing Synthesis","date":"2019-05-05T16:17:02.000Z","_content":"\nReal Time Singing Synthesizer project made from sinsy-NG. The idea was to generate vocal audio samples on real time easily for live coding performances. The code is [here](https://github.com/mathigatti/midi2voice).\n\n## Demo\n[Here](https://www.youtube.com/watch?v=wvbV75Tw_24) is a video demonstration using the program to synthesize samples and load them into the FoxDot live coding environment.\n\n[![IMAGE ALT TEXT HERE](https://i.ytimg.com/vi/wvbV75Tw_24/maxresdefault.jpg)](https://www.youtube.com/watch?v=wvbV75Tw_24)\n","source":"_posts/Singing-Synthesis.md","raw":"---\ntitle: Singing Synthesis\ndate: 2019-05-05 13:17:02\ntags:\n---\n\nReal Time Singing Synthesizer project made from sinsy-NG. The idea was to generate vocal audio samples on real time easily for live coding performances. The code is [here](https://github.com/mathigatti/midi2voice).\n\n## Demo\n[Here](https://www.youtube.com/watch?v=wvbV75Tw_24) is a video demonstration using the program to synthesize samples and load them into the FoxDot live coding environment.\n\n[![IMAGE ALT TEXT HERE](https://i.ytimg.com/vi/wvbV75Tw_24/maxresdefault.jpg)](https://www.youtube.com/watch?v=wvbV75Tw_24)\n","slug":"Singing-Synthesis","published":1,"updated":"2020-04-04T20:48:01.403Z","_id":"ck8hj9igf00034zry2cyocuoy","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Real Time Singing Synthesizer project made from sinsy-NG. The idea was to generate vocal audio samples on real time easily for live coding performances. The code is <a href=\"https://github.com/mathigatti/midi2voice\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h2 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h2><p><a href=\"https://www.youtube.com/watch?v=wvbV75Tw_24\" target=\"_blank\" rel=\"noopener\">Here</a> is a video demonstration using the program to synthesize samples and load them into the FoxDot live coding environment.</p>\n<p><a href=\"https://www.youtube.com/watch?v=wvbV75Tw_24\" target=\"_blank\" rel=\"noopener\"><img src=\"https://i.ytimg.com/vi/wvbV75Tw_24/maxresdefault.jpg\" alt=\"IMAGE ALT TEXT HERE\"></a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Real Time Singing Synthesizer project made from sinsy-NG. The idea was to generate vocal audio samples on real time easily for live coding performances. The code is <a href=\"https://github.com/mathigatti/midi2voice\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h2 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h2><p><a href=\"https://www.youtube.com/watch?v=wvbV75Tw_24\" target=\"_blank\" rel=\"noopener\">Here</a> is a video demonstration using the program to synthesize samples and load them into the FoxDot live coding environment.</p>\n<p><a href=\"https://www.youtube.com/watch?v=wvbV75Tw_24\" target=\"_blank\" rel=\"noopener\"><img src=\"https://i.ytimg.com/vi/wvbV75Tw_24/maxresdefault.jpg\" alt=\"IMAGE ALT TEXT HERE\"></a></p>\n"},{"title":"Audio Reactive Slime","date":"2019-12-31T14:06:33.000Z","_content":"\nA slime mold simulation made by nicoptere and modified by [solquemal](https://solquemal.com) and me for audio reactivity. Run demo [here](https://physarum.mathigatti.com/).\n\nInspired by this [amazing work](https://www.sagejenson.com/physarum). Implemented from [this paper](http://eprints.uwe.ac.uk/15260/1/artl.2010.16.2.pdf)\n\nYou can check a sample of this [here](https://www.instagram.com/p/B6ytj5rlUMf/?utm_source=ig_web_copy_link).\n\n![img0](physarum.jpg)\n![img1](physarum2.jpg)","source":"_posts/Audio-Reactive-Slime.md","raw":"---\ntitle: Audio Reactive Slime\ndate: 2019-12-31 11:06:33\ntags:\n---\n\nA slime mold simulation made by nicoptere and modified by [solquemal](https://solquemal.com) and me for audio reactivity. Run demo [here](https://physarum.mathigatti.com/).\n\nInspired by this [amazing work](https://www.sagejenson.com/physarum). Implemented from [this paper](http://eprints.uwe.ac.uk/15260/1/artl.2010.16.2.pdf)\n\nYou can check a sample of this [here](https://www.instagram.com/p/B6ytj5rlUMf/?utm_source=ig_web_copy_link).\n\n![img0](physarum.jpg)\n![img1](physarum2.jpg)","slug":"Audio-Reactive-Slime","published":1,"updated":"2020-04-04T20:54:35.912Z","_id":"ck8jb761d0000nlry8cv13tw7","comments":1,"layout":"post","photos":[],"link":"","content":"<p>A slime mold simulation made by nicoptere and modified by <a href=\"https://solquemal.com\" target=\"_blank\" rel=\"noopener\">solquemal</a> and me for audio reactivity. Run demo <a href=\"https://physarum.mathigatti.com/\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>Inspired by this <a href=\"https://www.sagejenson.com/physarum\" target=\"_blank\" rel=\"noopener\">amazing work</a>. Implemented from <a href=\"http://eprints.uwe.ac.uk/15260/1/artl.2010.16.2.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a></p>\n<p>You can check a sample of this <a href=\"https://www.instagram.com/p/B6ytj5rlUMf/?utm_source=ig_web_copy_link\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p><img src=\"physarum.jpg\" alt=\"img0\"><br><img src=\"physarum2.jpg\" alt=\"img1\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>A slime mold simulation made by nicoptere and modified by <a href=\"https://solquemal.com\" target=\"_blank\" rel=\"noopener\">solquemal</a> and me for audio reactivity. Run demo <a href=\"https://physarum.mathigatti.com/\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>Inspired by this <a href=\"https://www.sagejenson.com/physarum\" target=\"_blank\" rel=\"noopener\">amazing work</a>. Implemented from <a href=\"http://eprints.uwe.ac.uk/15260/1/artl.2010.16.2.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a></p>\n<p>You can check a sample of this <a href=\"https://www.instagram.com/p/B6ytj5rlUMf/?utm_source=ig_web_copy_link\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p><img src=\"physarum.jpg\" alt=\"img0\"><br><img src=\"physarum2.jpg\" alt=\"img1\"></p>\n"},{"title":"AI Poem Writer","date":"2020-01-21T14:10:27.000Z","_content":"\nPlaying around generating new texts using artificial intelligence algorithms (GPT-2 model) that are inspired on [beachslang](https://en.wikipedia.org/wiki/Beach_Slang) lyrics and spanish whatsapp conversations.\n\n<img src=\"english1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english3.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish3.jpg\" width=\"80%\" height=\"10\"/>\n\n","source":"_posts/AI-Poem-Writer.md","raw":"---\ntitle: AI Poem Writer\ndate: 2020-01-21 11:10:27\ntags: poetry, gpt-2\n---\n\nPlaying around generating new texts using artificial intelligence algorithms (GPT-2 model) that are inspired on [beachslang](https://en.wikipedia.org/wiki/Beach_Slang) lyrics and spanish whatsapp conversations.\n\n<img src=\"english1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english3.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish3.jpg\" width=\"80%\" height=\"10\"/>\n\n","slug":"AI-Poem-Writer","published":1,"updated":"2020-04-04T21:14:59.306Z","_id":"ck8jb761h0002nlryden35kvr","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Playing around generating new texts using artificial intelligence algorithms (GPT-2 model) that are inspired on <a href=\"https://en.wikipedia.org/wiki/Beach_Slang\" target=\"_blank\" rel=\"noopener\">beachslang</a> lyrics and spanish whatsapp conversations.</p>\n<img src=\"english1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english3.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish3.jpg\" width=\"80%\" height=\"10\"/>\n\n","site":{"data":{}},"excerpt":"","more":"<p>Playing around generating new texts using artificial intelligence algorithms (GPT-2 model) that are inspired on <a href=\"https://en.wikipedia.org/wiki/Beach_Slang\" target=\"_blank\" rel=\"noopener\">beachslang</a> lyrics and spanish whatsapp conversations.</p>\n<img src=\"english1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"english3.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish1.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish2.jpg\" width=\"80%\" height=\"10\"/>\n\n<img src=\"spanish3.jpg\" width=\"80%\" height=\"10\"/>\n\n"},{"title":"Midi to Voice","date":"2018-06-10T14:09:07.000Z","_content":"\nUsing the HMM-based Singing Voice Synthesis System from the Nagoya Institute of Technology I implented a [program](https://github.com/mathigatti/midi2voice) to convert music sheets in midi file format into voices that sing the specified notes. [Here](https://soundcloud.com/mathias-gatti/shallow-midi2voice) you can find a sample cover of Lady Gaga - Shallow.\n\n<a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\"><img src=\"shallow.jpg\" width=\"50%\" height=\"50%\"/></a>","source":"_posts/Midi-to-Voice.md","raw":"---\ntitle: Midi to Voice\ndate: 2018-06-10 11:09:07\ntags:\n---\n\nUsing the HMM-based Singing Voice Synthesis System from the Nagoya Institute of Technology I implented a [program](https://github.com/mathigatti/midi2voice) to convert music sheets in midi file format into voices that sing the specified notes. [Here](https://soundcloud.com/mathias-gatti/shallow-midi2voice) you can find a sample cover of Lady Gaga - Shallow.\n\n<a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\"><img src=\"shallow.jpg\" width=\"50%\" height=\"50%\"/></a>","slug":"Midi-to-Voice","published":1,"updated":"2020-04-04T20:45:50.152Z","_id":"ck8jb761i0003nlrybwo7gr1d","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Using the HMM-based Singing Voice Synthesis System from the Nagoya Institute of Technology I implented a <a href=\"https://github.com/mathigatti/midi2voice\" target=\"_blank\" rel=\"noopener\">program</a> to convert music sheets in midi file format into voices that sing the specified notes. <a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\" target=\"_blank\" rel=\"noopener\">Here</a> you can find a sample cover of Lady Gaga - Shallow.</p>\n<p><a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\" target=\"_blank\" rel=\"noopener\"><img src=\"shallow.jpg\" width=\"50%\" height=\"50%\"/></a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Using the HMM-based Singing Voice Synthesis System from the Nagoya Institute of Technology I implented a <a href=\"https://github.com/mathigatti/midi2voice\" target=\"_blank\" rel=\"noopener\">program</a> to convert music sheets in midi file format into voices that sing the specified notes. <a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\" target=\"_blank\" rel=\"noopener\">Here</a> you can find a sample cover of Lady Gaga - Shallow.</p>\n<p><a href=\"https://soundcloud.com/mathias-gatti/shallow-midi2voice\" target=\"_blank\" rel=\"noopener\"><img src=\"shallow.jpg\" width=\"50%\" height=\"50%\"/></a></p>\n"},{"title":"Normalized Google Distance","date":"2019-06-09T16:27:42.000Z","_content":"\nBased on the count of google results we can infer the popularity of a word. Also the relationship between the frequency of two words together with respect to its individual frequency is a useful measure of how much two words are related.\n\n![](https://ucarecdn.com/f8821813-5740-4332-ba60-2f5c474464f6/)\n\nBased on these ideas is defined the [Normalized Google distance](https://en.wikipedia.org/wiki/Normalized_Google_distance), in [this](https://www.codementor.io/@mathiasgatti/python-implementation-of-normalized-google-distance-simple-web-scraping-example-vrwwu5w58) post I show how to implement it in python using basic web scraping tools. The final code can be found [here](https://gist.github.com/mathigatti/aa12d484ad545e909e48bfa080a11eae).","source":"_posts/Normalized-Google-Distance.md","raw":"---\ntitle: Normalized Google Distance\ndate: 2019-06-09 13:27:42\ntags:\n---\n\nBased on the count of google results we can infer the popularity of a word. Also the relationship between the frequency of two words together with respect to its individual frequency is a useful measure of how much two words are related.\n\n![](https://ucarecdn.com/f8821813-5740-4332-ba60-2f5c474464f6/)\n\nBased on these ideas is defined the [Normalized Google distance](https://en.wikipedia.org/wiki/Normalized_Google_distance), in [this](https://www.codementor.io/@mathiasgatti/python-implementation-of-normalized-google-distance-simple-web-scraping-example-vrwwu5w58) post I show how to implement it in python using basic web scraping tools. The final code can be found [here](https://gist.github.com/mathigatti/aa12d484ad545e909e48bfa080a11eae).","slug":"Normalized-Google-Distance","published":1,"updated":"2020-04-04T21:29:43.196Z","_id":"ck8jb761j0004nlry05je9wjk","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Based on the count of google results we can infer the popularity of a word. Also the relationship between the frequency of two words together with respect to its individual frequency is a useful measure of how much two words are related.</p>\n<p><img src=\"https://ucarecdn.com/f8821813-5740-4332-ba60-2f5c474464f6/\" alt=\"\"></p>\n<p>Based on these ideas is defined the <a href=\"https://en.wikipedia.org/wiki/Normalized_Google_distance\" target=\"_blank\" rel=\"noopener\">Normalized Google distance</a>, in <a href=\"https://www.codementor.io/@mathiasgatti/python-implementation-of-normalized-google-distance-simple-web-scraping-example-vrwwu5w58\" target=\"_blank\" rel=\"noopener\">this</a> post I show how to implement it in python using basic web scraping tools. The final code can be found <a href=\"https://gist.github.com/mathigatti/aa12d484ad545e909e48bfa080a11eae\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Based on the count of google results we can infer the popularity of a word. Also the relationship between the frequency of two words together with respect to its individual frequency is a useful measure of how much two words are related.</p>\n<p><img src=\"https://ucarecdn.com/f8821813-5740-4332-ba60-2f5c474464f6/\" alt=\"\"></p>\n<p>Based on these ideas is defined the <a href=\"https://en.wikipedia.org/wiki/Normalized_Google_distance\" target=\"_blank\" rel=\"noopener\">Normalized Google distance</a>, in <a href=\"https://www.codementor.io/@mathiasgatti/python-implementation-of-normalized-google-distance-simple-web-scraping-example-vrwwu5w58\" target=\"_blank\" rel=\"noopener\">this</a> post I show how to implement it in python using basic web scraping tools. The final code can be found <a href=\"https://gist.github.com/mathigatti/aa12d484ad545e909e48bfa080a11eae\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n"},{"title":"Regenerative cellular automata","date":"2020-03-24T16:55:34.000Z","_content":"\nPlaying around with regenerative celullar automatas.\n\n<img src=\"automata1.gif\">\n<img src=\"automata2.gif\">\n\nWork inspired on the amazing work of [Mordvintsev, et al.](https://distill.pub/2020/growing-ca/).\n\n\n","source":"_posts/Regenerative-cellular-automata.md","raw":"---\ntitle: Regenerative cellular automata\ndate: 2020-03-24 13:55:34\ntags:\n---\n\nPlaying around with regenerative celullar automatas.\n\n<img src=\"automata1.gif\">\n<img src=\"automata2.gif\">\n\nWork inspired on the amazing work of [Mordvintsev, et al.](https://distill.pub/2020/growing-ca/).\n\n\n","slug":"Regenerative-cellular-automata","published":1,"updated":"2020-04-04T21:25:54.888Z","_id":"ck8jb761k0005nlry965w6k6l","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Playing around with regenerative celullar automatas.</p>\n<img src=\"automata1.gif\">\n<img src=\"automata2.gif\">\n\n<p>Work inspired on the amazing work of <a href=\"https://distill.pub/2020/growing-ca/\" target=\"_blank\" rel=\"noopener\">Mordvintsev, et al.</a>.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Playing around with regenerative celullar automatas.</p>\n<img src=\"automata1.gif\">\n<img src=\"automata2.gif\">\n\n<p>Work inspired on the amazing work of <a href=\"https://distill.pub/2020/growing-ca/\" target=\"_blank\" rel=\"noopener\">Mordvintsev, et al.</a>.</p>\n"},{"title":"Scraping formatted text from images","date":"2019-10-03T16:28:21.000Z","_content":"\n# Image to Text conversion utilities\n\nYou can find [here](https://github.com/mathigatti/img2txt) a small tokenization utility and examples of table extraction from images using Google Vision API. Google provides a good OCR to extract text from images but the output is not the best sometimes, in this repository I provide a simple postprocessing of the output in order to make it easier to use the API output.\n\n## Motivation\n\nGoogle OCR provides a text output which might not have the expected format, if that's the case it also provides a JSON output with information about the position of each recognized entity. The problem is that this data is not so well structured for some tasks, extracting tokens (Series of characters without spaces between each other) is not so easy with this JSON since it doesn't provides directly this information. The goal of this is to provide a way to postprocess this data into something more maneagable, so it's more appropiate for text processing tasks like extracting full lines of text or filtering words.\n\nIn order to do this a postprocessing code is provided at `src/image2tokens.py`. This is applied in order to extract tokens and then even more abstract concepts like text lines or table columns.\n\n## Demo\n\n### Sample Input\n![IMAGE ALT TEXT HERE](https://github.com/mathigatti/img2txt/blob/master/sample/input/sample.png?raw=true)\n\n### Sample Output\n```\n                          HR Information                                 Contact\n                                Position                                  Salary                                  Office                                   Extn.\n                              Accountant                                $162,700                                   Tokyo                                    5407\n           Chief Executive Officer (CEO)                              $1,200,000                                  London                                    5797\n                 Junior Technical Author                                 $86,000                           San Francisco                                    1562\n                       Software Engineer                                $132,000                                  London                                    2558\n                       Software Engineer                                $206,850                           San Francisco                                    1314\n                  Integration Specialist                                $372,000                                New York                                    4804\n                       Software Engineer                                $163,500                                  London                                    6222\n                       Pre-Sales Support                                $106,450                                New York                                    8330\n                         Sales Assistant                                $145,600                                New York                                    3990\n             Senior Javascript Developer                                $433,060                               Edinburgh                                    6224\n```","source":"_posts/Scraping-formatted-text-from-images.md","raw":"---\ntitle: Scraping formatted text from images\ndate: 2019-10-03 13:28:21\ntags: ocr\n---\n\n# Image to Text conversion utilities\n\nYou can find [here](https://github.com/mathigatti/img2txt) a small tokenization utility and examples of table extraction from images using Google Vision API. Google provides a good OCR to extract text from images but the output is not the best sometimes, in this repository I provide a simple postprocessing of the output in order to make it easier to use the API output.\n\n## Motivation\n\nGoogle OCR provides a text output which might not have the expected format, if that's the case it also provides a JSON output with information about the position of each recognized entity. The problem is that this data is not so well structured for some tasks, extracting tokens (Series of characters without spaces between each other) is not so easy with this JSON since it doesn't provides directly this information. The goal of this is to provide a way to postprocess this data into something more maneagable, so it's more appropiate for text processing tasks like extracting full lines of text or filtering words.\n\nIn order to do this a postprocessing code is provided at `src/image2tokens.py`. This is applied in order to extract tokens and then even more abstract concepts like text lines or table columns.\n\n## Demo\n\n### Sample Input\n![IMAGE ALT TEXT HERE](https://github.com/mathigatti/img2txt/blob/master/sample/input/sample.png?raw=true)\n\n### Sample Output\n```\n                          HR Information                                 Contact\n                                Position                                  Salary                                  Office                                   Extn.\n                              Accountant                                $162,700                                   Tokyo                                    5407\n           Chief Executive Officer (CEO)                              $1,200,000                                  London                                    5797\n                 Junior Technical Author                                 $86,000                           San Francisco                                    1562\n                       Software Engineer                                $132,000                                  London                                    2558\n                       Software Engineer                                $206,850                           San Francisco                                    1314\n                  Integration Specialist                                $372,000                                New York                                    4804\n                       Software Engineer                                $163,500                                  London                                    6222\n                       Pre-Sales Support                                $106,450                                New York                                    8330\n                         Sales Assistant                                $145,600                                New York                                    3990\n             Senior Javascript Developer                                $433,060                               Edinburgh                                    6224\n```","slug":"Scraping-formatted-text-from-images","published":1,"updated":"2020-04-04T21:24:42.881Z","_id":"ck8jb761l0006nlryb17b2n5h","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Image-to-Text-conversion-utilities\"><a href=\"#Image-to-Text-conversion-utilities\" class=\"headerlink\" title=\"Image to Text conversion utilities\"></a>Image to Text conversion utilities</h1><p>You can find <a href=\"https://github.com/mathigatti/img2txt\" target=\"_blank\" rel=\"noopener\">here</a> a small tokenization utility and examples of table extraction from images using Google Vision API. Google provides a good OCR to extract text from images but the output is not the best sometimes, in this repository I provide a simple postprocessing of the output in order to make it easier to use the API output.</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>Google OCR provides a text output which might not have the expected format, if that’s the case it also provides a JSON output with information about the position of each recognized entity. The problem is that this data is not so well structured for some tasks, extracting tokens (Series of characters without spaces between each other) is not so easy with this JSON since it doesn’t provides directly this information. The goal of this is to provide a way to postprocess this data into something more maneagable, so it’s more appropiate for text processing tasks like extracting full lines of text or filtering words.</p>\n<p>In order to do this a postprocessing code is provided at <code>src/image2tokens.py</code>. This is applied in order to extract tokens and then even more abstract concepts like text lines or table columns.</p>\n<h2 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h2><h3 id=\"Sample-Input\"><a href=\"#Sample-Input\" class=\"headerlink\" title=\"Sample Input\"></a>Sample Input</h3><p><img src=\"https://github.com/mathigatti/img2txt/blob/master/sample/input/sample.png?raw=true\" alt=\"IMAGE ALT TEXT HERE\"></p>\n<h3 id=\"Sample-Output\"><a href=\"#Sample-Output\" class=\"headerlink\" title=\"Sample Output\"></a>Sample Output</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">               HR Information                                 Contact</span><br><span class=\"line\">                     Position                                  Salary                                  Office                                   Extn.</span><br><span class=\"line\">                   Accountant                                $162,700                                   Tokyo                                    5407</span><br><span class=\"line\">Chief Executive Officer (CEO)                              $1,200,000                                  London                                    5797</span><br><span class=\"line\">      Junior Technical Author                                 $86,000                           San Francisco                                    1562</span><br><span class=\"line\">            Software Engineer                                $132,000                                  London                                    2558</span><br><span class=\"line\">            Software Engineer                                $206,850                           San Francisco                                    1314</span><br><span class=\"line\">       Integration Specialist                                $372,000                                New York                                    4804</span><br><span class=\"line\">            Software Engineer                                $163,500                                  London                                    6222</span><br><span class=\"line\">            Pre-Sales Support                                $106,450                                New York                                    8330</span><br><span class=\"line\">              Sales Assistant                                $145,600                                New York                                    3990</span><br><span class=\"line\">  Senior Javascript Developer                                $433,060                               Edinburgh                                    6224</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Image-to-Text-conversion-utilities\"><a href=\"#Image-to-Text-conversion-utilities\" class=\"headerlink\" title=\"Image to Text conversion utilities\"></a>Image to Text conversion utilities</h1><p>You can find <a href=\"https://github.com/mathigatti/img2txt\" target=\"_blank\" rel=\"noopener\">here</a> a small tokenization utility and examples of table extraction from images using Google Vision API. Google provides a good OCR to extract text from images but the output is not the best sometimes, in this repository I provide a simple postprocessing of the output in order to make it easier to use the API output.</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>Google OCR provides a text output which might not have the expected format, if that’s the case it also provides a JSON output with information about the position of each recognized entity. The problem is that this data is not so well structured for some tasks, extracting tokens (Series of characters without spaces between each other) is not so easy with this JSON since it doesn’t provides directly this information. The goal of this is to provide a way to postprocess this data into something more maneagable, so it’s more appropiate for text processing tasks like extracting full lines of text or filtering words.</p>\n<p>In order to do this a postprocessing code is provided at <code>src/image2tokens.py</code>. This is applied in order to extract tokens and then even more abstract concepts like text lines or table columns.</p>\n<h2 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h2><h3 id=\"Sample-Input\"><a href=\"#Sample-Input\" class=\"headerlink\" title=\"Sample Input\"></a>Sample Input</h3><p><img src=\"https://github.com/mathigatti/img2txt/blob/master/sample/input/sample.png?raw=true\" alt=\"IMAGE ALT TEXT HERE\"></p>\n<h3 id=\"Sample-Output\"><a href=\"#Sample-Output\" class=\"headerlink\" title=\"Sample Output\"></a>Sample Output</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">               HR Information                                 Contact</span><br><span class=\"line\">                     Position                                  Salary                                  Office                                   Extn.</span><br><span class=\"line\">                   Accountant                                $162,700                                   Tokyo                                    5407</span><br><span class=\"line\">Chief Executive Officer (CEO)                              $1,200,000                                  London                                    5797</span><br><span class=\"line\">      Junior Technical Author                                 $86,000                           San Francisco                                    1562</span><br><span class=\"line\">            Software Engineer                                $132,000                                  London                                    2558</span><br><span class=\"line\">            Software Engineer                                $206,850                           San Francisco                                    1314</span><br><span class=\"line\">       Integration Specialist                                $372,000                                New York                                    4804</span><br><span class=\"line\">            Software Engineer                                $163,500                                  London                                    6222</span><br><span class=\"line\">            Pre-Sales Support                                $106,450                                New York                                    8330</span><br><span class=\"line\">              Sales Assistant                                $145,600                                New York                                    3990</span><br><span class=\"line\">  Senior Javascript Developer                                $433,060                               Edinburgh                                    6224</span><br></pre></td></tr></table></figure>"},{"title":"Style Transfer Experiments","date":"2019-12-29T14:08:36.000Z","_content":"\nDeep convolutional neural networks were used to change the patterns on the original pictures.\n\n\n<img src=\"s1_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_2.jpg\" width=\"80%\" height=\"10\"/>\n","source":"_posts/Style-Transfer-Experiments.md","raw":"---\ntitle: Style Transfer Experiments\ndate: 2019-12-29 11:08:36\ntags: style-transfer\n---\n\nDeep convolutional neural networks were used to change the patterns on the original pictures.\n\n\n<img src=\"s1_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_2.jpg\" width=\"80%\" height=\"10\"/>\n","slug":"Style-Transfer-Experiments","published":1,"updated":"2020-04-04T21:37:29.799Z","_id":"ck8jb761m0007nlry4nk76xpp","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Deep convolutional neural networks were used to change the patterns on the original pictures.</p>\n<img src=\"s1_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_2.jpg\" width=\"80%\" height=\"10\"/>\n","site":{"data":{}},"excerpt":"","more":"<p>Deep convolutional neural networks were used to change the patterns on the original pictures.</p>\n<img src=\"s1_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s1_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_3.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s2_4.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s3_2.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_1.jpg\" width=\"80%\" height=\"10\"/>\n<img src=\"s4_2.jpg\" width=\"80%\" height=\"10\"/>\n"},{"title":"bu3nAmigue - Experimental Indie Band","date":"2019-12-15T14:09:50.000Z","_content":"\nOn december 2019, [solquemal](https://solquemal.com), [pablito](https://www.instagram.com/plabarta_/) and me formed bu3nAmigue, an artistic collective focused on live coding.\n\n[![IMAGE ALT TEXT HERE](https://i.ytimg.com/vi/stfLFoA8maM/maxresdefault.jpg)](https://www.youtube.com/watch?v=stfLFoA8maM)\n\nYou can check our social networks here:\n- [Instagram](https://www.instagram.com/bu3namigue/)\n- [bandcamp](https://bu3namigue.bandcamp.com/)\n- [YouTube](https://www.youtube.com/channel/UCnjUJE2RUee2IwyopOHQ6wg)\n- [GitHub](https://github.com/bu3namigue/)\n\n\n","source":"_posts/bu3nAmigue-Experimental-Indie-Band.md","raw":"---\ntitle: bu3nAmigue - Experimental Indie Band\ndate: 2019-12-15 11:09:50\ntags:\n---\n\nOn december 2019, [solquemal](https://solquemal.com), [pablito](https://www.instagram.com/plabarta_/) and me formed bu3nAmigue, an artistic collective focused on live coding.\n\n[![IMAGE ALT TEXT HERE](https://i.ytimg.com/vi/stfLFoA8maM/maxresdefault.jpg)](https://www.youtube.com/watch?v=stfLFoA8maM)\n\nYou can check our social networks here:\n- [Instagram](https://www.instagram.com/bu3namigue/)\n- [bandcamp](https://bu3namigue.bandcamp.com/)\n- [YouTube](https://www.youtube.com/channel/UCnjUJE2RUee2IwyopOHQ6wg)\n- [GitHub](https://github.com/bu3namigue/)\n\n\n","slug":"bu3nAmigue-Experimental-Indie-Band","published":1,"updated":"2020-04-04T21:04:33.796Z","_id":"ck8jb761m0008nlryaj7gb8pi","comments":1,"layout":"post","photos":[],"link":"","content":"<p>On december 2019, <a href=\"https://solquemal.com\" target=\"_blank\" rel=\"noopener\">solquemal</a>, <a href=\"https://www.instagram.com/plabarta_/\" target=\"_blank\" rel=\"noopener\">pablito</a> and me formed bu3nAmigue, an artistic collective focused on live coding.</p>\n<p><a href=\"https://www.youtube.com/watch?v=stfLFoA8maM\" target=\"_blank\" rel=\"noopener\"><img src=\"https://i.ytimg.com/vi/stfLFoA8maM/maxresdefault.jpg\" alt=\"IMAGE ALT TEXT HERE\"></a></p>\n<p>You can check our social networks here:</p>\n<ul>\n<li><a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">Instagram</a></li>\n<li><a href=\"https://bu3namigue.bandcamp.com/\" target=\"_blank\" rel=\"noopener\">bandcamp</a></li>\n<li><a href=\"https://www.youtube.com/channel/UCnjUJE2RUee2IwyopOHQ6wg\" target=\"_blank\" rel=\"noopener\">YouTube</a></li>\n<li><a href=\"https://github.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">GitHub</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>On december 2019, <a href=\"https://solquemal.com\" target=\"_blank\" rel=\"noopener\">solquemal</a>, <a href=\"https://www.instagram.com/plabarta_/\" target=\"_blank\" rel=\"noopener\">pablito</a> and me formed bu3nAmigue, an artistic collective focused on live coding.</p>\n<p><a href=\"https://www.youtube.com/watch?v=stfLFoA8maM\" target=\"_blank\" rel=\"noopener\"><img src=\"https://i.ytimg.com/vi/stfLFoA8maM/maxresdefault.jpg\" alt=\"IMAGE ALT TEXT HERE\"></a></p>\n<p>You can check our social networks here:</p>\n<ul>\n<li><a href=\"https://www.instagram.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">Instagram</a></li>\n<li><a href=\"https://bu3namigue.bandcamp.com/\" target=\"_blank\" rel=\"noopener\">bandcamp</a></li>\n<li><a href=\"https://www.youtube.com/channel/UCnjUJE2RUee2IwyopOHQ6wg\" target=\"_blank\" rel=\"noopener\">YouTube</a></li>\n<li><a href=\"https://github.com/bu3namigue/\" target=\"_blank\" rel=\"noopener\">GitHub</a></li>\n</ul>\n"},{"title":"Coding Psychological Experiments","date":"2019-06-09T21:39:02.000Z","_content":"\nPsychopy library is a useful framework to develop psychological experiments using Python. In this example I will show how to develop a basic experiment that registers how much time it takes for someone to press a key, then I will save the data as a csv file. The full code can be found [here](https://gist.github.com/mathigatti/3635a6414118e34fa90786fb67b6b7ea).\n\n![](https://ucarecdn.com/747786fb-53a6-470f-9d21-1c0285d2d320/)\n\n## The code\n\n### Libraries\nFirst I import the libraries\n```\nfrom psychopy import visual, core, event\nimport datetime # Used to register the date of the experiment\nimport pandas as pd # Used to save the data as csv easily\n```\n\n### Setting constants and global variables\n```\n# Colours\ngray = '#969696'\nblack = '#000000'\nwhite = '#FFFFFF'\n\n# Window parameters\nresolution = [300, 300]\n```\n\n### Defining main functions\n\n#### Window\n\nIn psychopy you define the window where all the screens are going to be displayed like this\n\n```\ndef window(resolution):\n    fullScreen = False\n    win = visual.Window(resolution,units=\"pix\",  color=gray, colorSpace='hex', fullscr=fullScreen, monitor = \"testMonitor\")\n    win.setMouseVisible(False)\n    return win\n```\n\n#### Screens\n\nWe are going to define the screens now. Here we will specify the text messages, its style, the background colour and all that kind of things. Our program has only two screens, the starter screen that ask you to start, and the stop screen that records how long you took to press the button.\n\n![WhatsApp Image 2019-06-09 at 11.40.49 (1).jpeg](https://ucarecdn.com/75931ec1-0f82-45f1-be76-22004d39dc4f/)\n\n```\ndef loadInstructionsAndFlip(win):\n    background = visual.Rect(win, width=resolution[0]+10, height=resolution[1]+10, fillColor=black, fillColorSpace='hex')\n    msg1 = visual.TextStim(win, text=\"press [ q ] to exit\", pos=(0.0,(-resolution[1]*0.10)), color=white, colorSpace='hex')\n    msg2 = visual.TextStim(win, text=\"press [ n ] to continue\", color=white, colorSpace='hex',alignHoriz='center', alignVert='center')\n    background.draw()\n    msg1.draw()\n    msg2.draw()\n\n    # Elements are only displayed after the flip command is executed\n    win.flip()\n\ndef loadStartScreenAndFlip(win):\n    background = visual.Rect(win, width=resolution[0]+10, height=resolution[1]+10, fillColor=gray, fillColorSpace='hex')\n    msg1 = visual.TextStim(win, text=\"press any key to start\", color=white, colorSpace='hex')\n    background.draw()\n    msg1.draw()\n\n    # Elements are only displayed after the flip command is executed\n    win.flip()\n```\n\n#### Main logic\nHere we implement the main logic of the program. We create the clock that measures the time. We reset it on every iteration and show each screen every time.\n```\ndef startScreensAndRecordData(win):\n    clock = core.Clock()\n    win.clearBuffer()\n\n    data = []\n    loadStartScreenAndFlip(win)\n    event.waitKeys()\n\n    while True:\n        loadInstructionsAndFlip(win)\n        clock.reset()\n        keys = event.waitKeys(keyList=[\"n\",\"q\"])\n        for key in keys:\n            time = clock.getTime()\n            print(\"You pressed the {} key on {} seconds\".format(key,round(time,3)))\n            data.append([key,time])\n            if key == \"q\":\n                return data\n            else:\n                loadStartScreenAndFlip(win)\n                event.waitKeys()\n```\n\n#### Puting all together and saving it\nFinally we put everything together and save the file as a CSV using pandas :)\n\n\n```\ndef main():\n    win = window(resolution)\n    data = startScreensAndRecordData(win)\n\n    pd.DataFrame(data,columns=[\"Key\",\"Time\"]).to_csv('experiment_' + str(datetime.date.today()) + '.csv')\n\n\n```\n\n![](https://ucarecdn.com/e0615065-b962-4851-998e-1d4da0ce1d75/)\n","source":"_posts/Coding-Psychological-Experiments.md","raw":"---\ntitle: Coding Psychological Experiments\ndate: 2019-06-09 18:39:02\ntags:\n---\n\nPsychopy library is a useful framework to develop psychological experiments using Python. In this example I will show how to develop a basic experiment that registers how much time it takes for someone to press a key, then I will save the data as a csv file. The full code can be found [here](https://gist.github.com/mathigatti/3635a6414118e34fa90786fb67b6b7ea).\n\n![](https://ucarecdn.com/747786fb-53a6-470f-9d21-1c0285d2d320/)\n\n## The code\n\n### Libraries\nFirst I import the libraries\n```\nfrom psychopy import visual, core, event\nimport datetime # Used to register the date of the experiment\nimport pandas as pd # Used to save the data as csv easily\n```\n\n### Setting constants and global variables\n```\n# Colours\ngray = '#969696'\nblack = '#000000'\nwhite = '#FFFFFF'\n\n# Window parameters\nresolution = [300, 300]\n```\n\n### Defining main functions\n\n#### Window\n\nIn psychopy you define the window where all the screens are going to be displayed like this\n\n```\ndef window(resolution):\n    fullScreen = False\n    win = visual.Window(resolution,units=\"pix\",  color=gray, colorSpace='hex', fullscr=fullScreen, monitor = \"testMonitor\")\n    win.setMouseVisible(False)\n    return win\n```\n\n#### Screens\n\nWe are going to define the screens now. Here we will specify the text messages, its style, the background colour and all that kind of things. Our program has only two screens, the starter screen that ask you to start, and the stop screen that records how long you took to press the button.\n\n![WhatsApp Image 2019-06-09 at 11.40.49 (1).jpeg](https://ucarecdn.com/75931ec1-0f82-45f1-be76-22004d39dc4f/)\n\n```\ndef loadInstructionsAndFlip(win):\n    background = visual.Rect(win, width=resolution[0]+10, height=resolution[1]+10, fillColor=black, fillColorSpace='hex')\n    msg1 = visual.TextStim(win, text=\"press [ q ] to exit\", pos=(0.0,(-resolution[1]*0.10)), color=white, colorSpace='hex')\n    msg2 = visual.TextStim(win, text=\"press [ n ] to continue\", color=white, colorSpace='hex',alignHoriz='center', alignVert='center')\n    background.draw()\n    msg1.draw()\n    msg2.draw()\n\n    # Elements are only displayed after the flip command is executed\n    win.flip()\n\ndef loadStartScreenAndFlip(win):\n    background = visual.Rect(win, width=resolution[0]+10, height=resolution[1]+10, fillColor=gray, fillColorSpace='hex')\n    msg1 = visual.TextStim(win, text=\"press any key to start\", color=white, colorSpace='hex')\n    background.draw()\n    msg1.draw()\n\n    # Elements are only displayed after the flip command is executed\n    win.flip()\n```\n\n#### Main logic\nHere we implement the main logic of the program. We create the clock that measures the time. We reset it on every iteration and show each screen every time.\n```\ndef startScreensAndRecordData(win):\n    clock = core.Clock()\n    win.clearBuffer()\n\n    data = []\n    loadStartScreenAndFlip(win)\n    event.waitKeys()\n\n    while True:\n        loadInstructionsAndFlip(win)\n        clock.reset()\n        keys = event.waitKeys(keyList=[\"n\",\"q\"])\n        for key in keys:\n            time = clock.getTime()\n            print(\"You pressed the {} key on {} seconds\".format(key,round(time,3)))\n            data.append([key,time])\n            if key == \"q\":\n                return data\n            else:\n                loadStartScreenAndFlip(win)\n                event.waitKeys()\n```\n\n#### Puting all together and saving it\nFinally we put everything together and save the file as a CSV using pandas :)\n\n\n```\ndef main():\n    win = window(resolution)\n    data = startScreensAndRecordData(win)\n\n    pd.DataFrame(data,columns=[\"Key\",\"Time\"]).to_csv('experiment_' + str(datetime.date.today()) + '.csv')\n\n\n```\n\n![](https://ucarecdn.com/e0615065-b962-4851-998e-1d4da0ce1d75/)\n","slug":"Coding-Psychological-Experiments","published":1,"updated":"2020-04-05T01:29:45.635Z","_id":"ck8m5369200068gryfyz8at8b","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Psychopy library is a useful framework to develop psychological experiments using Python. In this example I will show how to develop a basic experiment that registers how much time it takes for someone to press a key, then I will save the data as a csv file. The full code can be found <a href=\"https://gist.github.com/mathigatti/3635a6414118e34fa90786fb67b6b7ea\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p><img src=\"https://ucarecdn.com/747786fb-53a6-470f-9d21-1c0285d2d320/\" alt=\"\"></p>\n<h2 id=\"The-code\"><a href=\"#The-code\" class=\"headerlink\" title=\"The code\"></a>The code</h2><h3 id=\"Libraries\"><a href=\"#Libraries\" class=\"headerlink\" title=\"Libraries\"></a>Libraries</h3><p>First I import the libraries</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from psychopy import visual, core, event</span><br><span class=\"line\">import datetime # Used to register the date of the experiment</span><br><span class=\"line\">import pandas as pd # Used to save the data as csv easily</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Setting-constants-and-global-variables\"><a href=\"#Setting-constants-and-global-variables\" class=\"headerlink\" title=\"Setting constants and global variables\"></a>Setting constants and global variables</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Colours</span><br><span class=\"line\">gray &#x3D; &#39;#969696&#39;</span><br><span class=\"line\">black &#x3D; &#39;#000000&#39;</span><br><span class=\"line\">white &#x3D; &#39;#FFFFFF&#39;</span><br><span class=\"line\"></span><br><span class=\"line\"># Window parameters</span><br><span class=\"line\">resolution &#x3D; [300, 300]</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Defining-main-functions\"><a href=\"#Defining-main-functions\" class=\"headerlink\" title=\"Defining main functions\"></a>Defining main functions</h3><h4 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h4><p>In psychopy you define the window where all the screens are going to be displayed like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def window(resolution):</span><br><span class=\"line\">    fullScreen &#x3D; False</span><br><span class=\"line\">    win &#x3D; visual.Window(resolution,units&#x3D;&quot;pix&quot;,  color&#x3D;gray, colorSpace&#x3D;&#39;hex&#39;, fullscr&#x3D;fullScreen, monitor &#x3D; &quot;testMonitor&quot;)</span><br><span class=\"line\">    win.setMouseVisible(False)</span><br><span class=\"line\">    return win</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Screens\"><a href=\"#Screens\" class=\"headerlink\" title=\"Screens\"></a>Screens</h4><p>We are going to define the screens now. Here we will specify the text messages, its style, the background colour and all that kind of things. Our program has only two screens, the starter screen that ask you to start, and the stop screen that records how long you took to press the button.</p>\n<p><img src=\"https://ucarecdn.com/75931ec1-0f82-45f1-be76-22004d39dc4f/\" alt=\"WhatsApp Image 2019-06-09 at 11.40.49 (1).jpeg\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def loadInstructionsAndFlip(win):</span><br><span class=\"line\">    background &#x3D; visual.Rect(win, width&#x3D;resolution[0]+10, height&#x3D;resolution[1]+10, fillColor&#x3D;black, fillColorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg1 &#x3D; visual.TextStim(win, text&#x3D;&quot;press [ q ] to exit&quot;, pos&#x3D;(0.0,(-resolution[1]*0.10)), color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg2 &#x3D; visual.TextStim(win, text&#x3D;&quot;press [ n ] to continue&quot;, color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;,alignHoriz&#x3D;&#39;center&#39;, alignVert&#x3D;&#39;center&#39;)</span><br><span class=\"line\">    background.draw()</span><br><span class=\"line\">    msg1.draw()</span><br><span class=\"line\">    msg2.draw()</span><br><span class=\"line\"></span><br><span class=\"line\">    # Elements are only displayed after the flip command is executed</span><br><span class=\"line\">    win.flip()</span><br><span class=\"line\"></span><br><span class=\"line\">def loadStartScreenAndFlip(win):</span><br><span class=\"line\">    background &#x3D; visual.Rect(win, width&#x3D;resolution[0]+10, height&#x3D;resolution[1]+10, fillColor&#x3D;gray, fillColorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg1 &#x3D; visual.TextStim(win, text&#x3D;&quot;press any key to start&quot;, color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    background.draw()</span><br><span class=\"line\">    msg1.draw()</span><br><span class=\"line\"></span><br><span class=\"line\">    # Elements are only displayed after the flip command is executed</span><br><span class=\"line\">    win.flip()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Main-logic\"><a href=\"#Main-logic\" class=\"headerlink\" title=\"Main logic\"></a>Main logic</h4><p>Here we implement the main logic of the program. We create the clock that measures the time. We reset it on every iteration and show each screen every time.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def startScreensAndRecordData(win):</span><br><span class=\"line\">    clock &#x3D; core.Clock()</span><br><span class=\"line\">    win.clearBuffer()</span><br><span class=\"line\"></span><br><span class=\"line\">    data &#x3D; []</span><br><span class=\"line\">    loadStartScreenAndFlip(win)</span><br><span class=\"line\">    event.waitKeys()</span><br><span class=\"line\"></span><br><span class=\"line\">    while True:</span><br><span class=\"line\">        loadInstructionsAndFlip(win)</span><br><span class=\"line\">        clock.reset()</span><br><span class=\"line\">        keys &#x3D; event.waitKeys(keyList&#x3D;[&quot;n&quot;,&quot;q&quot;])</span><br><span class=\"line\">        for key in keys:</span><br><span class=\"line\">            time &#x3D; clock.getTime()</span><br><span class=\"line\">            print(&quot;You pressed the &#123;&#125; key on &#123;&#125; seconds&quot;.format(key,round(time,3)))</span><br><span class=\"line\">            data.append([key,time])</span><br><span class=\"line\">            if key &#x3D;&#x3D; &quot;q&quot;:</span><br><span class=\"line\">                return data</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                loadStartScreenAndFlip(win)</span><br><span class=\"line\">                event.waitKeys()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Puting-all-together-and-saving-it\"><a href=\"#Puting-all-together-and-saving-it\" class=\"headerlink\" title=\"Puting all together and saving it\"></a>Puting all together and saving it</h4><p>Finally we put everything together and save the file as a CSV using pandas :)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main():</span><br><span class=\"line\">    win &#x3D; window(resolution)</span><br><span class=\"line\">    data &#x3D; startScreensAndRecordData(win)</span><br><span class=\"line\"></span><br><span class=\"line\">    pd.DataFrame(data,columns&#x3D;[&quot;Key&quot;,&quot;Time&quot;]).to_csv(&#39;experiment_&#39; + str(datetime.date.today()) + &#39;.csv&#39;)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://ucarecdn.com/e0615065-b962-4851-998e-1d4da0ce1d75/\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Psychopy library is a useful framework to develop psychological experiments using Python. In this example I will show how to develop a basic experiment that registers how much time it takes for someone to press a key, then I will save the data as a csv file. The full code can be found <a href=\"https://gist.github.com/mathigatti/3635a6414118e34fa90786fb67b6b7ea\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p><img src=\"https://ucarecdn.com/747786fb-53a6-470f-9d21-1c0285d2d320/\" alt=\"\"></p>\n<h2 id=\"The-code\"><a href=\"#The-code\" class=\"headerlink\" title=\"The code\"></a>The code</h2><h3 id=\"Libraries\"><a href=\"#Libraries\" class=\"headerlink\" title=\"Libraries\"></a>Libraries</h3><p>First I import the libraries</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from psychopy import visual, core, event</span><br><span class=\"line\">import datetime # Used to register the date of the experiment</span><br><span class=\"line\">import pandas as pd # Used to save the data as csv easily</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Setting-constants-and-global-variables\"><a href=\"#Setting-constants-and-global-variables\" class=\"headerlink\" title=\"Setting constants and global variables\"></a>Setting constants and global variables</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Colours</span><br><span class=\"line\">gray &#x3D; &#39;#969696&#39;</span><br><span class=\"line\">black &#x3D; &#39;#000000&#39;</span><br><span class=\"line\">white &#x3D; &#39;#FFFFFF&#39;</span><br><span class=\"line\"></span><br><span class=\"line\"># Window parameters</span><br><span class=\"line\">resolution &#x3D; [300, 300]</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Defining-main-functions\"><a href=\"#Defining-main-functions\" class=\"headerlink\" title=\"Defining main functions\"></a>Defining main functions</h3><h4 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h4><p>In psychopy you define the window where all the screens are going to be displayed like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def window(resolution):</span><br><span class=\"line\">    fullScreen &#x3D; False</span><br><span class=\"line\">    win &#x3D; visual.Window(resolution,units&#x3D;&quot;pix&quot;,  color&#x3D;gray, colorSpace&#x3D;&#39;hex&#39;, fullscr&#x3D;fullScreen, monitor &#x3D; &quot;testMonitor&quot;)</span><br><span class=\"line\">    win.setMouseVisible(False)</span><br><span class=\"line\">    return win</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Screens\"><a href=\"#Screens\" class=\"headerlink\" title=\"Screens\"></a>Screens</h4><p>We are going to define the screens now. Here we will specify the text messages, its style, the background colour and all that kind of things. Our program has only two screens, the starter screen that ask you to start, and the stop screen that records how long you took to press the button.</p>\n<p><img src=\"https://ucarecdn.com/75931ec1-0f82-45f1-be76-22004d39dc4f/\" alt=\"WhatsApp Image 2019-06-09 at 11.40.49 (1).jpeg\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def loadInstructionsAndFlip(win):</span><br><span class=\"line\">    background &#x3D; visual.Rect(win, width&#x3D;resolution[0]+10, height&#x3D;resolution[1]+10, fillColor&#x3D;black, fillColorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg1 &#x3D; visual.TextStim(win, text&#x3D;&quot;press [ q ] to exit&quot;, pos&#x3D;(0.0,(-resolution[1]*0.10)), color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg2 &#x3D; visual.TextStim(win, text&#x3D;&quot;press [ n ] to continue&quot;, color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;,alignHoriz&#x3D;&#39;center&#39;, alignVert&#x3D;&#39;center&#39;)</span><br><span class=\"line\">    background.draw()</span><br><span class=\"line\">    msg1.draw()</span><br><span class=\"line\">    msg2.draw()</span><br><span class=\"line\"></span><br><span class=\"line\">    # Elements are only displayed after the flip command is executed</span><br><span class=\"line\">    win.flip()</span><br><span class=\"line\"></span><br><span class=\"line\">def loadStartScreenAndFlip(win):</span><br><span class=\"line\">    background &#x3D; visual.Rect(win, width&#x3D;resolution[0]+10, height&#x3D;resolution[1]+10, fillColor&#x3D;gray, fillColorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    msg1 &#x3D; visual.TextStim(win, text&#x3D;&quot;press any key to start&quot;, color&#x3D;white, colorSpace&#x3D;&#39;hex&#39;)</span><br><span class=\"line\">    background.draw()</span><br><span class=\"line\">    msg1.draw()</span><br><span class=\"line\"></span><br><span class=\"line\">    # Elements are only displayed after the flip command is executed</span><br><span class=\"line\">    win.flip()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Main-logic\"><a href=\"#Main-logic\" class=\"headerlink\" title=\"Main logic\"></a>Main logic</h4><p>Here we implement the main logic of the program. We create the clock that measures the time. We reset it on every iteration and show each screen every time.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def startScreensAndRecordData(win):</span><br><span class=\"line\">    clock &#x3D; core.Clock()</span><br><span class=\"line\">    win.clearBuffer()</span><br><span class=\"line\"></span><br><span class=\"line\">    data &#x3D; []</span><br><span class=\"line\">    loadStartScreenAndFlip(win)</span><br><span class=\"line\">    event.waitKeys()</span><br><span class=\"line\"></span><br><span class=\"line\">    while True:</span><br><span class=\"line\">        loadInstructionsAndFlip(win)</span><br><span class=\"line\">        clock.reset()</span><br><span class=\"line\">        keys &#x3D; event.waitKeys(keyList&#x3D;[&quot;n&quot;,&quot;q&quot;])</span><br><span class=\"line\">        for key in keys:</span><br><span class=\"line\">            time &#x3D; clock.getTime()</span><br><span class=\"line\">            print(&quot;You pressed the &#123;&#125; key on &#123;&#125; seconds&quot;.format(key,round(time,3)))</span><br><span class=\"line\">            data.append([key,time])</span><br><span class=\"line\">            if key &#x3D;&#x3D; &quot;q&quot;:</span><br><span class=\"line\">                return data</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                loadStartScreenAndFlip(win)</span><br><span class=\"line\">                event.waitKeys()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Puting-all-together-and-saving-it\"><a href=\"#Puting-all-together-and-saving-it\" class=\"headerlink\" title=\"Puting all together and saving it\"></a>Puting all together and saving it</h4><p>Finally we put everything together and save the file as a CSV using pandas :)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def main():</span><br><span class=\"line\">    win &#x3D; window(resolution)</span><br><span class=\"line\">    data &#x3D; startScreensAndRecordData(win)</span><br><span class=\"line\"></span><br><span class=\"line\">    pd.DataFrame(data,columns&#x3D;[&quot;Key&quot;,&quot;Time&quot;]).to_csv(&#39;experiment_&#39; + str(datetime.date.today()) + &#39;.csv&#39;)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://ucarecdn.com/e0615065-b962-4851-998e-1d4da0ce1d75/\" alt=\"\"></p>\n"},{"title":"Computing brain connectivity using portable devices - Master's Thesis","date":"2018-04-04T21:40:41.000Z","_content":"\nThe topic of my master's thesis was neuroscience. I studied how to use low cost portable electroenchephalographs for advanced  connectivity studies. [Here](https://github.com/mathigatti/EmotivExperiments) is the code for the experiments I coded and [here](https://github.com/mathigatti/EmotivClassifier) the classifiers I used in my thesis to infer different states based on neural connectivity. These things together with the great work of Marcos Pietto and his team ended up in the publications of [this](https://www.ncbi.nlm.nih.gov/pubmed/30475814) paper.\n\n![](emotiv.jpg)\n\n## Abstract\n\nThis is the abstract of my thesis, you can read the full version [here](http://dc.sigedep.exactas.uba.ar/media/academic/grade/thesis/Tesis_Mathias_Gatti.pdf) (It's in Spanish).\n\n<i>\nElectroencephalography (EEG) studies have shown to be a fundamental tool in the understanding of human cognitive processes and as a complementary method for clinical diagnosis. EEG recordings are usually restricted to the lab’s environment, which is an important limitation to the study of populations that are difficult to move to the lab or that this environment causes an effect by itself (such as little naturalness or stress). Recently, new EEG equipment at a significant lower cost and greater portability emerged mainly for gaming. However, the use of this equipment could open the possibility in the long term of application of different clinical and research protocols in more ecologically valid environments.\n\nIt is possible to build networks from the EEG recordings by looking at the similarity between the different electrodes, but there are many ways to define this similarity. In this thesis, we aimed to compare the resulting network by applying different measures, in terms of their robustness and their ability to predict different attributes of the subjects.\n\nThus, in the first place, we developed an experimentation environment that allowed both data collection in the lab and massive data collection in the schools. Two data sets were analyzed, the first including the low-cost EEG and the high-resolution EEG in adults, and the second with only the low-cost EEG in preschool children at the schools.\n\nThe results showed that some similarity measures are robust enough to analyze brain activity at the level of networks using the Emotiv system. In particular, we performed different experiments using the adult dataset, of which the PLV (Phase Locking value) and the correlation measures (both Spearman and Pearson) consistently gave the best results. Then, the preschool children dataset were analyzed using these measures and some individual variables, such as sleep level, sex and the distinction between open and closed eyes, were successfully predicted (AUC>0.67). As a final step, the code was released online, allowing anyone to replicate the data collection and connectivity analysis with the Emotiv system.\n\nThis thesis paves the way for the massive recording of brain activity with different paradigms and, in particular, the study of changes in brain connectivity throughout development, in more ecologically valid environments.\n</i>","source":"_posts/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis.md","raw":"---\ntitle: Computing brain connectivity using portable devices - Master's Thesis\ndate: 2018-04-04 18:40:41\ntags: neuroscience\n---\n\nThe topic of my master's thesis was neuroscience. I studied how to use low cost portable electroenchephalographs for advanced  connectivity studies. [Here](https://github.com/mathigatti/EmotivExperiments) is the code for the experiments I coded and [here](https://github.com/mathigatti/EmotivClassifier) the classifiers I used in my thesis to infer different states based on neural connectivity. These things together with the great work of Marcos Pietto and his team ended up in the publications of [this](https://www.ncbi.nlm.nih.gov/pubmed/30475814) paper.\n\n![](emotiv.jpg)\n\n## Abstract\n\nThis is the abstract of my thesis, you can read the full version [here](http://dc.sigedep.exactas.uba.ar/media/academic/grade/thesis/Tesis_Mathias_Gatti.pdf) (It's in Spanish).\n\n<i>\nElectroencephalography (EEG) studies have shown to be a fundamental tool in the understanding of human cognitive processes and as a complementary method for clinical diagnosis. EEG recordings are usually restricted to the lab’s environment, which is an important limitation to the study of populations that are difficult to move to the lab or that this environment causes an effect by itself (such as little naturalness or stress). Recently, new EEG equipment at a significant lower cost and greater portability emerged mainly for gaming. However, the use of this equipment could open the possibility in the long term of application of different clinical and research protocols in more ecologically valid environments.\n\nIt is possible to build networks from the EEG recordings by looking at the similarity between the different electrodes, but there are many ways to define this similarity. In this thesis, we aimed to compare the resulting network by applying different measures, in terms of their robustness and their ability to predict different attributes of the subjects.\n\nThus, in the first place, we developed an experimentation environment that allowed both data collection in the lab and massive data collection in the schools. Two data sets were analyzed, the first including the low-cost EEG and the high-resolution EEG in adults, and the second with only the low-cost EEG in preschool children at the schools.\n\nThe results showed that some similarity measures are robust enough to analyze brain activity at the level of networks using the Emotiv system. In particular, we performed different experiments using the adult dataset, of which the PLV (Phase Locking value) and the correlation measures (both Spearman and Pearson) consistently gave the best results. Then, the preschool children dataset were analyzed using these measures and some individual variables, such as sleep level, sex and the distinction between open and closed eyes, were successfully predicted (AUC>0.67). As a final step, the code was released online, allowing anyone to replicate the data collection and connectivity analysis with the Emotiv system.\n\nThis thesis paves the way for the massive recording of brain activity with different paradigms and, in particular, the study of changes in brain connectivity throughout development, in more ecologically valid environments.\n</i>","slug":"Computing-brain-connectivity-using-portable-devices-Master-s-Thesis","published":1,"updated":"2020-04-05T01:32:50.974Z","_id":"ck8m55ato00078gry1hg55s8h","comments":1,"layout":"post","photos":[],"link":"","content":"<p>The topic of my master’s thesis was neuroscience. I studied how to use low cost portable electroenchephalographs for advanced  connectivity studies. <a href=\"https://github.com/mathigatti/EmotivExperiments\" target=\"_blank\" rel=\"noopener\">Here</a> is the code for the experiments I coded and <a href=\"https://github.com/mathigatti/EmotivClassifier\" target=\"_blank\" rel=\"noopener\">here</a> the classifiers I used in my thesis to infer different states based on neural connectivity. These things together with the great work of Marcos Pietto and his team ended up in the publications of <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/30475814\" target=\"_blank\" rel=\"noopener\">this</a> paper.</p>\n<p><img src=\"emotiv.jpg\" alt=\"\"></p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This is the abstract of my thesis, you can read the full version <a href=\"http://dc.sigedep.exactas.uba.ar/media/academic/grade/thesis/Tesis_Mathias_Gatti.pdf\" target=\"_blank\" rel=\"noopener\">here</a> (It’s in Spanish).</p>\n<i>\nElectroencephalography (EEG) studies have shown to be a fundamental tool in the understanding of human cognitive processes and as a complementary method for clinical diagnosis. EEG recordings are usually restricted to the lab’s environment, which is an important limitation to the study of populations that are difficult to move to the lab or that this environment causes an effect by itself (such as little naturalness or stress). Recently, new EEG equipment at a significant lower cost and greater portability emerged mainly for gaming. However, the use of this equipment could open the possibility in the long term of application of different clinical and research protocols in more ecologically valid environments.\n\n<p>It is possible to build networks from the EEG recordings by looking at the similarity between the different electrodes, but there are many ways to define this similarity. In this thesis, we aimed to compare the resulting network by applying different measures, in terms of their robustness and their ability to predict different attributes of the subjects.</p>\n<p>Thus, in the first place, we developed an experimentation environment that allowed both data collection in the lab and massive data collection in the schools. Two data sets were analyzed, the first including the low-cost EEG and the high-resolution EEG in adults, and the second with only the low-cost EEG in preschool children at the schools.</p>\n<p>The results showed that some similarity measures are robust enough to analyze brain activity at the level of networks using the Emotiv system. In particular, we performed different experiments using the adult dataset, of which the PLV (Phase Locking value) and the correlation measures (both Spearman and Pearson) consistently gave the best results. Then, the preschool children dataset were analyzed using these measures and some individual variables, such as sleep level, sex and the distinction between open and closed eyes, were successfully predicted (AUC&gt;0.67). As a final step, the code was released online, allowing anyone to replicate the data collection and connectivity analysis with the Emotiv system.</p>\n<p>This thesis paves the way for the massive recording of brain activity with different paradigms and, in particular, the study of changes in brain connectivity throughout development, in more ecologically valid environments.<br></i></p>\n","site":{"data":{}},"excerpt":"","more":"<p>The topic of my master’s thesis was neuroscience. I studied how to use low cost portable electroenchephalographs for advanced  connectivity studies. <a href=\"https://github.com/mathigatti/EmotivExperiments\" target=\"_blank\" rel=\"noopener\">Here</a> is the code for the experiments I coded and <a href=\"https://github.com/mathigatti/EmotivClassifier\" target=\"_blank\" rel=\"noopener\">here</a> the classifiers I used in my thesis to infer different states based on neural connectivity. These things together with the great work of Marcos Pietto and his team ended up in the publications of <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/30475814\" target=\"_blank\" rel=\"noopener\">this</a> paper.</p>\n<p><img src=\"emotiv.jpg\" alt=\"\"></p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This is the abstract of my thesis, you can read the full version <a href=\"http://dc.sigedep.exactas.uba.ar/media/academic/grade/thesis/Tesis_Mathias_Gatti.pdf\" target=\"_blank\" rel=\"noopener\">here</a> (It’s in Spanish).</p>\n<i>\nElectroencephalography (EEG) studies have shown to be a fundamental tool in the understanding of human cognitive processes and as a complementary method for clinical diagnosis. EEG recordings are usually restricted to the lab’s environment, which is an important limitation to the study of populations that are difficult to move to the lab or that this environment causes an effect by itself (such as little naturalness or stress). Recently, new EEG equipment at a significant lower cost and greater portability emerged mainly for gaming. However, the use of this equipment could open the possibility in the long term of application of different clinical and research protocols in more ecologically valid environments.\n\n<p>It is possible to build networks from the EEG recordings by looking at the similarity between the different electrodes, but there are many ways to define this similarity. In this thesis, we aimed to compare the resulting network by applying different measures, in terms of their robustness and their ability to predict different attributes of the subjects.</p>\n<p>Thus, in the first place, we developed an experimentation environment that allowed both data collection in the lab and massive data collection in the schools. Two data sets were analyzed, the first including the low-cost EEG and the high-resolution EEG in adults, and the second with only the low-cost EEG in preschool children at the schools.</p>\n<p>The results showed that some similarity measures are robust enough to analyze brain activity at the level of networks using the Emotiv system. In particular, we performed different experiments using the adult dataset, of which the PLV (Phase Locking value) and the correlation measures (both Spearman and Pearson) consistently gave the best results. Then, the preschool children dataset were analyzed using these measures and some individual variables, such as sleep level, sex and the distinction between open and closed eyes, were successfully predicted (AUC&gt;0.67). As a final step, the code was released online, allowing anyone to replicate the data collection and connectivity analysis with the Emotiv system.</p>\n<p>This thesis paves the way for the massive recording of brain activity with different paradigms and, in particular, the study of changes in brain connectivity throughout development, in more ecologically valid environments.<br></i></p>\n"},{"title":"Looking for the formula of beauty","date":"2019-08-05T21:41:46.000Z","_content":"\nSome time ago I created [this](https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f) small script to convert numbers into patterns. I'm not going to explain how the script works in detail but it's inspired on [Stephen Wolfram's Elementary Cellular Automatas](https://en.wikipedia.org/wiki/Elementary_cellular_automaton) which converts numbers like 30 into binary (00011110) and then interprets the digits as turning ON or OFF of 8 different basic rules (In that case there are 4 rules activated, rule 4, 5, 6 and 7) that define when to turn ON and OFF a pixel in the image.\n\n![ElementaryCARule030_1000.gif](https://ucarecdn.com/a4323adb-4c13-4c4f-a32d-97f86468a1f0/)\n\nUsing this I can generate an infinite number of different patterns, the problem is that most of them are not really interesting and I have no time to check them one by one. That's why in this post I explain how I tried to automate the process of finding out the most interesting/beautiful cellular automatas.\n\n## Clusterization\nMy goal is to group the patterns by its beauty. I do this using a clustering algorithm based on features frequently attributed to beauty such as fractal dimensionality and compression efficiency. You can read more about these features here: [Forsythe, Alex, et al. \"Predict\n\n(https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art).\n\n## The Code\nThe full code is [here](https://github.com/mathigatti/CellularAutomataClassification) but I also uploaded it into colab [here](https://colab.research.google.com/drive/1FFNRZuRW7lkKi1LnbMR-d8KI0EADl871) so you can run everything from your web browser.\n\n### Defining clustering attributes\nFirst I define the previously mentioned attributes, fractal dimension (Code taken from [here](https://gist.github.com/rougier/e5eafc276a4e54f516ed5559df4242c0)) and compression score (The weight of a raw tiff image over its weight compressed as a gif image).\n\n```\nfrom fractaldimension import fractal_dimension\nimport cv2\nimport os\n\ndef fractalDimension(number):\n    im = cv2.imread('images/'+str(number)+'.tiff', cv2.IMREAD_GRAYSCALE)\n    newDimension = fractal_dimension(im, 0.9)\n    return newDimension\n\ndef compressionScore(number):\n    statinfo = os.stat('images/'+str(number)+'.gif')\n    gif = statinfo.st_size # size of the file\n    \n    statinfo = os.stat('images/'+str(number)+'.tiff')\n    tiff = statinfo.st_size # size of the file\n    \n    return tiff/gif\n```\n\n### Clustering\nThere are several clustering algorithms, you can choose the one that best fits your use case.\n\n![sphx_glr_plot_cluster_comparison_001.png](https://ucarecdn.com/a6d86443-9072-44f0-b032-ccede2fe4073/)\n\nIn my case I ended up using Agglomerative Clustering which captures better the clusters generated by this dataset.\n\nYou need to specify the number of clusters, I tried with different numbers, at the end I chose 5 since it grouped them well from null patterns to crazy and chaotic ones.\n\n```\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# Applying clustering algorithm\nclustering = AgglomerativeClustering(n_clusters=5).fit(df[['Fractal Dimension','Compression Eficciency']].values)\ndf[\"cluster\"] = clustering.labels_\n\n# Plotting results\nfig, ax = plt.subplots()\ncmap = cm.get_cmap('gist_rainbow')\nax = df.plot(kind='scatter', x='Fractal Dimension', y='Compression Eficciency',cmap=cmap, c='cluster',ax=ax)\nplt.show()\n```\n![descarga (5).png](https://ucarecdn.com/4fe8f003-2d3f-4296-8c24-9283bb587e2b/)\n\n## Results\nHere I show some samples of each cluster. I sorted them from the simplest ones to the most complex. As you can see this method is useful to identify and discard uninteresting patterns such us the ones from the Cluster 0. It's also useful to identify the most beautiful patterns, most of the best patterns I found are from the Cluster 3, the one with big complexity but not the biggest fractal dimension.\n\n### Cluster 0 (Null patterns)\n![descarga.png](https://ucarecdn.com/6f41e3ce-45dd-4b8e-aa00-b96fde1f09b4/)\n\n### Cluster 1\n![descarga (1).png](https://ucarecdn.com/565971db-1047-4100-92aa-c4feca3697ef/)\n\n### Cluster 2\n![descarga (2).png](https://ucarecdn.com/98775929-3cdc-4ff7-8dbb-0038663896bd/)\n\n### Cluster 3\n![descarga.png](https://ucarecdn.com/8f4a6dcf-b501-4b8a-bcc0-0b4822b6c26e/)\n\n### Cluster 4 (Crazy and chaotic patterns)\n![descarga (1).png](https://ucarecdn.com/b7066ecc-a054-4d34-9ec6-e9ef521fad85/)","source":"_posts/Looking-for-the-beauty-formula.md","raw":"---\ntitle: Looking for the formula of beauty\ndate: 2019-08-05 18:41:46\ntags: cellular-automata\n---\n\nSome time ago I created [this](https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f) small script to convert numbers into patterns. I'm not going to explain how the script works in detail but it's inspired on [Stephen Wolfram's Elementary Cellular Automatas](https://en.wikipedia.org/wiki/Elementary_cellular_automaton) which converts numbers like 30 into binary (00011110) and then interprets the digits as turning ON or OFF of 8 different basic rules (In that case there are 4 rules activated, rule 4, 5, 6 and 7) that define when to turn ON and OFF a pixel in the image.\n\n![ElementaryCARule030_1000.gif](https://ucarecdn.com/a4323adb-4c13-4c4f-a32d-97f86468a1f0/)\n\nUsing this I can generate an infinite number of different patterns, the problem is that most of them are not really interesting and I have no time to check them one by one. That's why in this post I explain how I tried to automate the process of finding out the most interesting/beautiful cellular automatas.\n\n## Clusterization\nMy goal is to group the patterns by its beauty. I do this using a clustering algorithm based on features frequently attributed to beauty such as fractal dimensionality and compression efficiency. You can read more about these features here: [Forsythe, Alex, et al. \"Predict\n\n(https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art).\n\n## The Code\nThe full code is [here](https://github.com/mathigatti/CellularAutomataClassification) but I also uploaded it into colab [here](https://colab.research.google.com/drive/1FFNRZuRW7lkKi1LnbMR-d8KI0EADl871) so you can run everything from your web browser.\n\n### Defining clustering attributes\nFirst I define the previously mentioned attributes, fractal dimension (Code taken from [here](https://gist.github.com/rougier/e5eafc276a4e54f516ed5559df4242c0)) and compression score (The weight of a raw tiff image over its weight compressed as a gif image).\n\n```\nfrom fractaldimension import fractal_dimension\nimport cv2\nimport os\n\ndef fractalDimension(number):\n    im = cv2.imread('images/'+str(number)+'.tiff', cv2.IMREAD_GRAYSCALE)\n    newDimension = fractal_dimension(im, 0.9)\n    return newDimension\n\ndef compressionScore(number):\n    statinfo = os.stat('images/'+str(number)+'.gif')\n    gif = statinfo.st_size # size of the file\n    \n    statinfo = os.stat('images/'+str(number)+'.tiff')\n    tiff = statinfo.st_size # size of the file\n    \n    return tiff/gif\n```\n\n### Clustering\nThere are several clustering algorithms, you can choose the one that best fits your use case.\n\n![sphx_glr_plot_cluster_comparison_001.png](https://ucarecdn.com/a6d86443-9072-44f0-b032-ccede2fe4073/)\n\nIn my case I ended up using Agglomerative Clustering which captures better the clusters generated by this dataset.\n\nYou need to specify the number of clusters, I tried with different numbers, at the end I chose 5 since it grouped them well from null patterns to crazy and chaotic ones.\n\n```\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# Applying clustering algorithm\nclustering = AgglomerativeClustering(n_clusters=5).fit(df[['Fractal Dimension','Compression Eficciency']].values)\ndf[\"cluster\"] = clustering.labels_\n\n# Plotting results\nfig, ax = plt.subplots()\ncmap = cm.get_cmap('gist_rainbow')\nax = df.plot(kind='scatter', x='Fractal Dimension', y='Compression Eficciency',cmap=cmap, c='cluster',ax=ax)\nplt.show()\n```\n![descarga (5).png](https://ucarecdn.com/4fe8f003-2d3f-4296-8c24-9283bb587e2b/)\n\n## Results\nHere I show some samples of each cluster. I sorted them from the simplest ones to the most complex. As you can see this method is useful to identify and discard uninteresting patterns such us the ones from the Cluster 0. It's also useful to identify the most beautiful patterns, most of the best patterns I found are from the Cluster 3, the one with big complexity but not the biggest fractal dimension.\n\n### Cluster 0 (Null patterns)\n![descarga.png](https://ucarecdn.com/6f41e3ce-45dd-4b8e-aa00-b96fde1f09b4/)\n\n### Cluster 1\n![descarga (1).png](https://ucarecdn.com/565971db-1047-4100-92aa-c4feca3697ef/)\n\n### Cluster 2\n![descarga (2).png](https://ucarecdn.com/98775929-3cdc-4ff7-8dbb-0038663896bd/)\n\n### Cluster 3\n![descarga.png](https://ucarecdn.com/8f4a6dcf-b501-4b8a-bcc0-0b4822b6c26e/)\n\n### Cluster 4 (Crazy and chaotic patterns)\n![descarga (1).png](https://ucarecdn.com/b7066ecc-a054-4d34-9ec6-e9ef521fad85/)","slug":"Looking-for-the-beauty-formula","published":1,"updated":"2020-04-05T01:25:03.310Z","_id":"ck8m56ono00088gry11wsdl2c","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Some time ago I created <a href=\"https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f\" target=\"_blank\" rel=\"noopener\">this</a> small script to convert numbers into patterns. I’m not going to explain how the script works in detail but it’s inspired on <a href=\"https://en.wikipedia.org/wiki/Elementary_cellular_automaton\" target=\"_blank\" rel=\"noopener\">Stephen Wolfram’s Elementary Cellular Automatas</a> which converts numbers like 30 into binary (00011110) and then interprets the digits as turning ON or OFF of 8 different basic rules (In that case there are 4 rules activated, rule 4, 5, 6 and 7) that define when to turn ON and OFF a pixel in the image.</p>\n<p><img src=\"https://ucarecdn.com/a4323adb-4c13-4c4f-a32d-97f86468a1f0/\" alt=\"ElementaryCARule030_1000.gif\"></p>\n<p>Using this I can generate an infinite number of different patterns, the problem is that most of them are not really interesting and I have no time to check them one by one. That’s why in this post I explain how I tried to automate the process of finding out the most interesting/beautiful cellular automatas.</p>\n<h2 id=\"Clusterization\"><a href=\"#Clusterization\" class=\"headerlink\" title=\"Clusterization\"></a>Clusterization</h2><p>My goal is to group the patterns by its beauty. I do this using a clustering algorithm based on features frequently attributed to beauty such as fractal dimensionality and compression efficiency. You can read more about these features here: [Forsythe, Alex, et al. “Predict</p>\n<p>(<a href=\"https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art\" target=\"_blank\" rel=\"noopener\">https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art</a>).</p>\n<h2 id=\"The-Code\"><a href=\"#The-Code\" class=\"headerlink\" title=\"The Code\"></a>The Code</h2><p>The full code is <a href=\"https://github.com/mathigatti/CellularAutomataClassification\" target=\"_blank\" rel=\"noopener\">here</a> but I also uploaded it into colab <a href=\"https://colab.research.google.com/drive/1FFNRZuRW7lkKi1LnbMR-d8KI0EADl871\" target=\"_blank\" rel=\"noopener\">here</a> so you can run everything from your web browser.</p>\n<h3 id=\"Defining-clustering-attributes\"><a href=\"#Defining-clustering-attributes\" class=\"headerlink\" title=\"Defining clustering attributes\"></a>Defining clustering attributes</h3><p>First I define the previously mentioned attributes, fractal dimension (Code taken from <a href=\"https://gist.github.com/rougier/e5eafc276a4e54f516ed5559df4242c0\" target=\"_blank\" rel=\"noopener\">here</a>) and compression score (The weight of a raw tiff image over its weight compressed as a gif image).</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from fractaldimension import fractal_dimension</span><br><span class=\"line\">import cv2</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">def fractalDimension(number):</span><br><span class=\"line\">    im &#x3D; cv2.imread(&#39;images&#x2F;&#39;+str(number)+&#39;.tiff&#39;, cv2.IMREAD_GRAYSCALE)</span><br><span class=\"line\">    newDimension &#x3D; fractal_dimension(im, 0.9)</span><br><span class=\"line\">    return newDimension</span><br><span class=\"line\"></span><br><span class=\"line\">def compressionScore(number):</span><br><span class=\"line\">    statinfo &#x3D; os.stat(&#39;images&#x2F;&#39;+str(number)+&#39;.gif&#39;)</span><br><span class=\"line\">    gif &#x3D; statinfo.st_size # size of the file</span><br><span class=\"line\">    </span><br><span class=\"line\">    statinfo &#x3D; os.stat(&#39;images&#x2F;&#39;+str(number)+&#39;.tiff&#39;)</span><br><span class=\"line\">    tiff &#x3D; statinfo.st_size # size of the file</span><br><span class=\"line\">    </span><br><span class=\"line\">    return tiff&#x2F;gif</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Clustering\"><a href=\"#Clustering\" class=\"headerlink\" title=\"Clustering\"></a>Clustering</h3><p>There are several clustering algorithms, you can choose the one that best fits your use case.</p>\n<p><img src=\"https://ucarecdn.com/a6d86443-9072-44f0-b032-ccede2fe4073/\" alt=\"sphx_glr_plot_cluster_comparison_001.png\"></p>\n<p>In my case I ended up using Agglomerative Clustering which captures better the clusters generated by this dataset.</p>\n<p>You need to specify the number of clusters, I tried with different numbers, at the end I chose 5 since it grouped them well from null patterns to crazy and chaotic ones.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.cluster import AgglomerativeClustering</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib import cm</span><br><span class=\"line\"></span><br><span class=\"line\"># Applying clustering algorithm</span><br><span class=\"line\">clustering &#x3D; AgglomerativeClustering(n_clusters&#x3D;5).fit(df[[&#39;Fractal Dimension&#39;,&#39;Compression Eficciency&#39;]].values)</span><br><span class=\"line\">df[&quot;cluster&quot;] &#x3D; clustering.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"># Plotting results</span><br><span class=\"line\">fig, ax &#x3D; plt.subplots()</span><br><span class=\"line\">cmap &#x3D; cm.get_cmap(&#39;gist_rainbow&#39;)</span><br><span class=\"line\">ax &#x3D; df.plot(kind&#x3D;&#39;scatter&#39;, x&#x3D;&#39;Fractal Dimension&#39;, y&#x3D;&#39;Compression Eficciency&#39;,cmap&#x3D;cmap, c&#x3D;&#39;cluster&#39;,ax&#x3D;ax)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://ucarecdn.com/4fe8f003-2d3f-4296-8c24-9283bb587e2b/\" alt=\"descarga (5).png\"></p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Here I show some samples of each cluster. I sorted them from the simplest ones to the most complex. As you can see this method is useful to identify and discard uninteresting patterns such us the ones from the Cluster 0. It’s also useful to identify the most beautiful patterns, most of the best patterns I found are from the Cluster 3, the one with big complexity but not the biggest fractal dimension.</p>\n<h3 id=\"Cluster-0-Null-patterns\"><a href=\"#Cluster-0-Null-patterns\" class=\"headerlink\" title=\"Cluster 0 (Null patterns)\"></a>Cluster 0 (Null patterns)</h3><p><img src=\"https://ucarecdn.com/6f41e3ce-45dd-4b8e-aa00-b96fde1f09b4/\" alt=\"descarga.png\"></p>\n<h3 id=\"Cluster-1\"><a href=\"#Cluster-1\" class=\"headerlink\" title=\"Cluster 1\"></a>Cluster 1</h3><p><img src=\"https://ucarecdn.com/565971db-1047-4100-92aa-c4feca3697ef/\" alt=\"descarga (1).png\"></p>\n<h3 id=\"Cluster-2\"><a href=\"#Cluster-2\" class=\"headerlink\" title=\"Cluster 2\"></a>Cluster 2</h3><p><img src=\"https://ucarecdn.com/98775929-3cdc-4ff7-8dbb-0038663896bd/\" alt=\"descarga (2).png\"></p>\n<h3 id=\"Cluster-3\"><a href=\"#Cluster-3\" class=\"headerlink\" title=\"Cluster 3\"></a>Cluster 3</h3><p><img src=\"https://ucarecdn.com/8f4a6dcf-b501-4b8a-bcc0-0b4822b6c26e/\" alt=\"descarga.png\"></p>\n<h3 id=\"Cluster-4-Crazy-and-chaotic-patterns\"><a href=\"#Cluster-4-Crazy-and-chaotic-patterns\" class=\"headerlink\" title=\"Cluster 4 (Crazy and chaotic patterns)\"></a>Cluster 4 (Crazy and chaotic patterns)</h3><p><img src=\"https://ucarecdn.com/b7066ecc-a054-4d34-9ec6-e9ef521fad85/\" alt=\"descarga (1).png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Some time ago I created <a href=\"https://gist.github.com/mathigatti/439a0e81556f2698c7db4f41189d201f\" target=\"_blank\" rel=\"noopener\">this</a> small script to convert numbers into patterns. I’m not going to explain how the script works in detail but it’s inspired on <a href=\"https://en.wikipedia.org/wiki/Elementary_cellular_automaton\" target=\"_blank\" rel=\"noopener\">Stephen Wolfram’s Elementary Cellular Automatas</a> which converts numbers like 30 into binary (00011110) and then interprets the digits as turning ON or OFF of 8 different basic rules (In that case there are 4 rules activated, rule 4, 5, 6 and 7) that define when to turn ON and OFF a pixel in the image.</p>\n<p><img src=\"https://ucarecdn.com/a4323adb-4c13-4c4f-a32d-97f86468a1f0/\" alt=\"ElementaryCARule030_1000.gif\"></p>\n<p>Using this I can generate an infinite number of different patterns, the problem is that most of them are not really interesting and I have no time to check them one by one. That’s why in this post I explain how I tried to automate the process of finding out the most interesting/beautiful cellular automatas.</p>\n<h2 id=\"Clusterization\"><a href=\"#Clusterization\" class=\"headerlink\" title=\"Clusterization\"></a>Clusterization</h2><p>My goal is to group the patterns by its beauty. I do this using a clustering algorithm based on features frequently attributed to beauty such as fractal dimensionality and compression efficiency. You can read more about these features here: [Forsythe, Alex, et al. “Predict</p>\n<p>(<a href=\"https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art\" target=\"_blank\" rel=\"noopener\">https://www.researchgate.net/publication/49761486_Predicting_beauty_Fractal_dimension_and_visual_complexity_in_art</a>).</p>\n<h2 id=\"The-Code\"><a href=\"#The-Code\" class=\"headerlink\" title=\"The Code\"></a>The Code</h2><p>The full code is <a href=\"https://github.com/mathigatti/CellularAutomataClassification\" target=\"_blank\" rel=\"noopener\">here</a> but I also uploaded it into colab <a href=\"https://colab.research.google.com/drive/1FFNRZuRW7lkKi1LnbMR-d8KI0EADl871\" target=\"_blank\" rel=\"noopener\">here</a> so you can run everything from your web browser.</p>\n<h3 id=\"Defining-clustering-attributes\"><a href=\"#Defining-clustering-attributes\" class=\"headerlink\" title=\"Defining clustering attributes\"></a>Defining clustering attributes</h3><p>First I define the previously mentioned attributes, fractal dimension (Code taken from <a href=\"https://gist.github.com/rougier/e5eafc276a4e54f516ed5559df4242c0\" target=\"_blank\" rel=\"noopener\">here</a>) and compression score (The weight of a raw tiff image over its weight compressed as a gif image).</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from fractaldimension import fractal_dimension</span><br><span class=\"line\">import cv2</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">def fractalDimension(number):</span><br><span class=\"line\">    im &#x3D; cv2.imread(&#39;images&#x2F;&#39;+str(number)+&#39;.tiff&#39;, cv2.IMREAD_GRAYSCALE)</span><br><span class=\"line\">    newDimension &#x3D; fractal_dimension(im, 0.9)</span><br><span class=\"line\">    return newDimension</span><br><span class=\"line\"></span><br><span class=\"line\">def compressionScore(number):</span><br><span class=\"line\">    statinfo &#x3D; os.stat(&#39;images&#x2F;&#39;+str(number)+&#39;.gif&#39;)</span><br><span class=\"line\">    gif &#x3D; statinfo.st_size # size of the file</span><br><span class=\"line\">    </span><br><span class=\"line\">    statinfo &#x3D; os.stat(&#39;images&#x2F;&#39;+str(number)+&#39;.tiff&#39;)</span><br><span class=\"line\">    tiff &#x3D; statinfo.st_size # size of the file</span><br><span class=\"line\">    </span><br><span class=\"line\">    return tiff&#x2F;gif</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Clustering\"><a href=\"#Clustering\" class=\"headerlink\" title=\"Clustering\"></a>Clustering</h3><p>There are several clustering algorithms, you can choose the one that best fits your use case.</p>\n<p><img src=\"https://ucarecdn.com/a6d86443-9072-44f0-b032-ccede2fe4073/\" alt=\"sphx_glr_plot_cluster_comparison_001.png\"></p>\n<p>In my case I ended up using Agglomerative Clustering which captures better the clusters generated by this dataset.</p>\n<p>You need to specify the number of clusters, I tried with different numbers, at the end I chose 5 since it grouped them well from null patterns to crazy and chaotic ones.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.cluster import AgglomerativeClustering</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib import cm</span><br><span class=\"line\"></span><br><span class=\"line\"># Applying clustering algorithm</span><br><span class=\"line\">clustering &#x3D; AgglomerativeClustering(n_clusters&#x3D;5).fit(df[[&#39;Fractal Dimension&#39;,&#39;Compression Eficciency&#39;]].values)</span><br><span class=\"line\">df[&quot;cluster&quot;] &#x3D; clustering.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"># Plotting results</span><br><span class=\"line\">fig, ax &#x3D; plt.subplots()</span><br><span class=\"line\">cmap &#x3D; cm.get_cmap(&#39;gist_rainbow&#39;)</span><br><span class=\"line\">ax &#x3D; df.plot(kind&#x3D;&#39;scatter&#39;, x&#x3D;&#39;Fractal Dimension&#39;, y&#x3D;&#39;Compression Eficciency&#39;,cmap&#x3D;cmap, c&#x3D;&#39;cluster&#39;,ax&#x3D;ax)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://ucarecdn.com/4fe8f003-2d3f-4296-8c24-9283bb587e2b/\" alt=\"descarga (5).png\"></p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Here I show some samples of each cluster. I sorted them from the simplest ones to the most complex. As you can see this method is useful to identify and discard uninteresting patterns such us the ones from the Cluster 0. It’s also useful to identify the most beautiful patterns, most of the best patterns I found are from the Cluster 3, the one with big complexity but not the biggest fractal dimension.</p>\n<h3 id=\"Cluster-0-Null-patterns\"><a href=\"#Cluster-0-Null-patterns\" class=\"headerlink\" title=\"Cluster 0 (Null patterns)\"></a>Cluster 0 (Null patterns)</h3><p><img src=\"https://ucarecdn.com/6f41e3ce-45dd-4b8e-aa00-b96fde1f09b4/\" alt=\"descarga.png\"></p>\n<h3 id=\"Cluster-1\"><a href=\"#Cluster-1\" class=\"headerlink\" title=\"Cluster 1\"></a>Cluster 1</h3><p><img src=\"https://ucarecdn.com/565971db-1047-4100-92aa-c4feca3697ef/\" alt=\"descarga (1).png\"></p>\n<h3 id=\"Cluster-2\"><a href=\"#Cluster-2\" class=\"headerlink\" title=\"Cluster 2\"></a>Cluster 2</h3><p><img src=\"https://ucarecdn.com/98775929-3cdc-4ff7-8dbb-0038663896bd/\" alt=\"descarga (2).png\"></p>\n<h3 id=\"Cluster-3\"><a href=\"#Cluster-3\" class=\"headerlink\" title=\"Cluster 3\"></a>Cluster 3</h3><p><img src=\"https://ucarecdn.com/8f4a6dcf-b501-4b8a-bcc0-0b4822b6c26e/\" alt=\"descarga.png\"></p>\n<h3 id=\"Cluster-4-Crazy-and-chaotic-patterns\"><a href=\"#Cluster-4-Crazy-and-chaotic-patterns\" class=\"headerlink\" title=\"Cluster 4 (Crazy and chaotic patterns)\"></a>Cluster 4 (Crazy and chaotic patterns)</h3><p><img src=\"https://ucarecdn.com/b7066ecc-a054-4d34-9ec6-e9ef521fad85/\" alt=\"descarga (1).png\"></p>\n"},{"title":"Scraping drum patterns from PDF","date":"2020-02-20T21:48:17.000Z","_content":"\nPocket JSON Ops is a rhythm dictionary taken from the book [Pocket Operations](https://b.shittyrecording.studio/file/shittyrec/print/Pocket+Operations+(2019-07-01).pdf), which compiles different percussion rhythms.\n\n![alt text](https://github.com/bu3nAmigue/pocket-json-ops/raw/master/pattern_example.jpg)\n\n\n## Why we did this?\n\nIn order to use the different rhythms compiled with different livecoding languages, convert the PDF into a beautiful JSON for the modern music computer. This was generated from an HTML version, generated from the PDF, and then analyzed using the Python Beautiful Soup 4 library.\n\n## How to listen the patterns?\n\nThe `dpattern2foxdot.py` script contains an example of how patterns can be read and reproduced within FoxDot, Python's live music framework.","source":"_posts/Scraping-drum-patterns-from-PDF.md","raw":"---\ntitle: Scraping drum patterns from PDF\ndate: 2020-02-20 18:48:17\ntags:\n---\n\nPocket JSON Ops is a rhythm dictionary taken from the book [Pocket Operations](https://b.shittyrecording.studio/file/shittyrec/print/Pocket+Operations+(2019-07-01).pdf), which compiles different percussion rhythms.\n\n![alt text](https://github.com/bu3nAmigue/pocket-json-ops/raw/master/pattern_example.jpg)\n\n\n## Why we did this?\n\nIn order to use the different rhythms compiled with different livecoding languages, convert the PDF into a beautiful JSON for the modern music computer. This was generated from an HTML version, generated from the PDF, and then analyzed using the Python Beautiful Soup 4 library.\n\n## How to listen the patterns?\n\nThe `dpattern2foxdot.py` script contains an example of how patterns can be read and reproduced within FoxDot, Python's live music framework.","slug":"Scraping-drum-patterns-from-PDF","published":1,"updated":"2020-04-05T01:30:43.296Z","_id":"ck8m5f2h2000b8gry1scq4eik","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Pocket JSON Ops is a rhythm dictionary taken from the book <a href=\"https://b.shittyrecording.studio/file/shittyrec/print/Pocket+Operations+(2019-07-01).pdf\" target=\"_blank\" rel=\"noopener\">Pocket Operations</a>, which compiles different percussion rhythms.</p>\n<p><img src=\"https://github.com/bu3nAmigue/pocket-json-ops/raw/master/pattern_example.jpg\" alt=\"alt text\"></p>\n<h2 id=\"Why-we-did-this\"><a href=\"#Why-we-did-this\" class=\"headerlink\" title=\"Why we did this?\"></a>Why we did this?</h2><p>In order to use the different rhythms compiled with different livecoding languages, convert the PDF into a beautiful JSON for the modern music computer. This was generated from an HTML version, generated from the PDF, and then analyzed using the Python Beautiful Soup 4 library.</p>\n<h2 id=\"How-to-listen-the-patterns\"><a href=\"#How-to-listen-the-patterns\" class=\"headerlink\" title=\"How to listen the patterns?\"></a>How to listen the patterns?</h2><p>The <code>dpattern2foxdot.py</code> script contains an example of how patterns can be read and reproduced within FoxDot, Python’s live music framework.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Pocket JSON Ops is a rhythm dictionary taken from the book <a href=\"https://b.shittyrecording.studio/file/shittyrec/print/Pocket+Operations+(2019-07-01).pdf\" target=\"_blank\" rel=\"noopener\">Pocket Operations</a>, which compiles different percussion rhythms.</p>\n<p><img src=\"https://github.com/bu3nAmigue/pocket-json-ops/raw/master/pattern_example.jpg\" alt=\"alt text\"></p>\n<h2 id=\"Why-we-did-this\"><a href=\"#Why-we-did-this\" class=\"headerlink\" title=\"Why we did this?\"></a>Why we did this?</h2><p>In order to use the different rhythms compiled with different livecoding languages, convert the PDF into a beautiful JSON for the modern music computer. This was generated from an HTML version, generated from the PDF, and then analyzed using the Python Beautiful Soup 4 library.</p>\n<h2 id=\"How-to-listen-the-patterns\"><a href=\"#How-to-listen-the-patterns\" class=\"headerlink\" title=\"How to listen the patterns?\"></a>How to listen the patterns?</h2><p>The <code>dpattern2foxdot.py</code> script contains an example of how patterns can be read and reproduced within FoxDot, Python’s live music framework.</p>\n"}],"PostAsset":[{"_id":"source/_posts/number2image/pattern.png","slug":"pattern.png","post":"ck8hi88zu000085ry9tkg2461","modified":0,"renderable":0},{"_id":"source/_posts/jardin-sonoro/inicio.png","slug":"inicio.png","post":"ck8hj4b4c00024zry4lyp1yut","modified":0,"renderable":0},{"_id":"source/_posts/jardin-sonoro/load.png","slug":"load.png","post":"ck8hj4b4c00024zry4lyp1yut","modified":0,"renderable":0},{"_id":"source/_posts/jardin-sonoro/player.png","slug":"player.png","post":"ck8hj4b4c00024zry4lyp1yut","modified":0,"renderable":0},{"_id":"source/_posts/Midi-to-Voice/shallow.jpg","slug":"shallow.jpg","post":"ck8jb761i0003nlrybwo7gr1d","modified":0,"renderable":0},{"_id":"source/_posts/Audio-Reactive-Slime/physarum.jpg","slug":"physarum.jpg","post":"ck8jb761d0000nlry8cv13tw7","modified":0,"renderable":0},{"_id":"source/_posts/Audio-Reactive-Slime/physarum2.jpg","slug":"physarum2.jpg","post":"ck8jb761d0000nlry8cv13tw7","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/english3.jpg","slug":"english3.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/english2.jpg","slug":"english2.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/english1.jpg","slug":"english1.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/spanish3.jpg","slug":"spanish3.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/spanish2.jpg","slug":"spanish2.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/AI-Poem-Writer/spanish1.jpg","slug":"spanish1.jpg","post":"ck8jb761h0002nlryden35kvr","modified":0,"renderable":0},{"_id":"source/_posts/Regenerative-cellular-automata/automata1.gif","slug":"automata1.gif","post":"ck8jb761k0005nlry965w6k6l","modified":0,"renderable":0},{"_id":"source/_posts/Regenerative-cellular-automata/automata2.gif","slug":"automata2.gif","post":"ck8jb761k0005nlry965w6k6l","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s1_2.jpg","slug":"s1_2.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s1_3.jpg","slug":"s1_3.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s1_4.jpg","slug":"s1_4.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s2_1.jpg","slug":"s2_1.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s2_2.jpg","slug":"s2_2.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s2_3.jpg","slug":"s2_3.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s2_4.jpg","slug":"s2_4.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s3_1.jpg","slug":"s3_1.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s3_2.jpg","slug":"s3_2.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s4_1.jpg","slug":"s4_1.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s4_2.jpg","slug":"s4_2.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Style-Transfer-Experiments/s1_1.jpg","slug":"s1_1.jpg","post":"ck8jb761m0007nlry4nk76xpp","modified":0,"renderable":0},{"_id":"source/_posts/Computing-brain-connectivity-using-portable-devices-Master-s-Thesis/emotiv.jpg","slug":"emotiv.jpg","post":"ck8m55ato00078gry1hg55s8h","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ck8jb761h0002nlryden35kvr","tag_id":"ck8m46kjc00008gry85m0dmyj","_id":"ck8m46kje00018gryd32fb6wj"},{"post_id":"ck8jb761l0006nlryb17b2n5h","tag_id":"ck8m4kqvi00028gry0vu6hecu","_id":"ck8m4kqvk00038gry76a70dto"},{"post_id":"ck8jb761m0007nlry4nk76xpp","tag_id":"ck8m500pq00048gry8eofb5oz","_id":"ck8m500pr00058gryaj04fcwj"},{"post_id":"ck8m56ono00088gry11wsdl2c","tag_id":"ck8m5cdl200098gry42nz5eep","_id":"ck8m5cdl3000a8grybu8ea4ig"},{"post_id":"ck8m55ato00078gry1hg55s8h","tag_id":"ck8m5yjdl000c8gryghmoeebd","_id":"ck8m5yjdm000d8gryad0h18k5"}],"Tag":[{"name":"poetry, gpt-2","_id":"ck8m46kjc00008gry85m0dmyj"},{"name":"ocr","_id":"ck8m4kqvi00028gry0vu6hecu"},{"name":"style-transfer","_id":"ck8m500pq00048gry8eofb5oz"},{"name":"cellular-automata","_id":"ck8m5cdl200098gry42nz5eep"},{"name":"neuroscience","_id":"ck8m5yjdl000c8gryghmoeebd"}]}}